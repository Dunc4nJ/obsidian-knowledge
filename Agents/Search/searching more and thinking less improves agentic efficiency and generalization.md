---
created: 2026-02-28
description: Prioritizing search breadth over deep reasoning yields more efficient and generalizable long-horizon agent behavior.
source: https://arxiv.org/abs/2602.22675
type: paper
authors:
  - Qianben Chen
  - Tianrui Qin
  - King Zhu
  - Qiexiang Wang
  - Chengjun Yu
  - Shu Xu
  - Xin Gui
  - Jingyi Cao
  - Jiaqi Wu
  - He Zhu
  - Tiannan Wang
  - Jiayu Zhang
  - Xinpeng Liu
  - Jian Yang
  - Jiaheng Liu
  - Piaohong Wang
  - Dingfeng Shi
  - Minghao Liu
  - Yuchen Eleanor Jiang
  - Yuqing Wang
  - Maojia Song
  - Tianyu Zheng
  - Ge Zhang
  - Wangchunshu Zhou
arxiv: "2602.22675"
---

## Abstract

Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, the authors propose Search More, Think Less (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, they introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task-appropriate evaluation metrics. They train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state-of-the-art performance across benchmarks including BrowseComp (48.6%), GAIA (75.7%), Xbench (82.0%), and DeepResearch Bench (45.9%). Compared to MiroThinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7%, while improving accuracy.

## Key Takeaways

1. **Parallel beats sequential for agentic search.** The core architectural insight is that replacing sequential, one-tool-per-turn reasoning with parallel subtask execution (3.5 tool calls per step on average) dramatically improves efficiency — 70% fewer reasoning steps on BrowseComp with higher accuracy. This directly validates what [[the research is clear - coding agents are bottlenecked by search not coding ability]] argues: search organization, not reasoning depth, is the bottleneck.

2. **Search breadth scales better than reasoning depth.** Increasing retrieval top-k (number of URLs per search query) from 4 to 20 consistently improves performance under fixed interaction budgets, while gains from deeper reasoning chains show diminishing returns. The "search more, think less" principle is a concrete scaling axis for agent design.

3. **Plan-driven context management enables longer effective trajectories.** When the 128K context window fills up, SMTL performs a forced plan refinement, drops all pre-plan context, and continues from the refreshed plan. This plan-centric reset preserves task structure without losing progress — a practical solution to the context overflow problem that plagues long-horizon agents.

4. **Unified data pipeline bridges deep search and deep research.** The subgraph-based data construction pipeline generates both deterministic QA tasks (with verifiable answers) and open-ended research questions from the same knowledge graph structure. This yields a single 30B-parameter agent that performs competitively on both BrowseComp-style factual search and DeepResearch Bench report synthesis — a generalization result most prior approaches can't match.

5. **Failed trajectories exhaust budgets; successful ones converge early.** Analysis shows that median steps of successful cases stay flat as max interaction budget increases, while failed cases hit the ceiling. This means additional compute primarily helps hard cases that need more exploration, not easy cases — a useful insight for adaptive budget allocation in [[intelligent ai delegation requires trust accountability and adaptive monitoring not just task decomposition|multi-agent orchestration]].

6. **SFT + RL training recipe with trajectory curation matters.** The paper uses distilled trajectories from DeepSeek-V3.2 and GPT-5, filtered by context length (≤64K), interaction efficiency (≥3 tool calls/step), and trajectory length (shortest correct). The RL stage uses modified RLOO with negative trajectory filtering to avoid learning from environmental failures — a practical training recipe for [[async rl from real conversations lets agents continuously improve without blocking inference|agent RL systems]].

![[chen-22675-fig-001.jpeg]]
![[chen-22675-fig-003.jpeg]]

## External Resources

- **Code, models, datasets:** Referenced as "Open Source" in paper header (OPPO AI Agent Team)
- **Contact:** Wangchunshu Zhou — zhouwangchunshu@oppo.com
- **Backbone model:** Qwen3-30B-A3B-Instruct-2507
- **Tools used:** Serper API (web search), Jina Reader API (page crawling), DeepSeek-V3.2 (summarization)

## Original Content

> [!quote]- Full Paper Text
> 
> 
> # Search More, Think Less: Rethinking Long-Horizon Agentic Search for Efficiency and Generalization
> 
> OPPO AI Agent Team
> 
> ## **Abstract**
> 
> Recent deep research agents primarily improve performance by scaling reasoning depth, but this leads to high inference cost and latency in search-intensive scenarios. Moreover, generalization across heterogeneous research settings remains challenging. In this work, we propose Search More, Think Less (SMTL), a framework for long-horizon agentic search that targets both efficiency and generalization. SMTL replaces sequential reasoning with parallel evidence acquisition, enabling efficient context management under constrained context budgets. To support generalization across task types, we further introduce a unified data synthesis pipeline that constructs search tasks spanning both deterministic question answering and open-ended research scenarios with task appropriate evaluation metrics. We train an end-to-end agent using supervised fine-tuning and reinforcement learning, achieving strong and often state of the art performance across benchmarks including BrowseComp (48.6%), GAIA (75.7%), Xbench (82.0%), and DeepResearch Bench (45.9%). Compared to Mirothinker-v1.0, SMTL with maximum 100 interaction steps reduces the average number of reasoning steps on BrowseComp by 70.7%, while improving accuracy.
> 
> Date: February 27, 2026
> 
> Open Source: Code Amodels Datasets
> 
> Correspondence: Wangchunshu Zhou at zhouwangchunshu@oppo.com
> 
> <span id="page-0-0"></span>![[chen-22675-fig-001.jpeg]]
> 
> 
> 
> ![[chen-22675-fig-002.jpeg]]
> 
> **(b)** Generalization across multiple benchmarks.
> 
> **Figure 1 Overview of SMTL Performance.** (a) Efficiency on BrowseComp. All methods are evaluated with their default inference settings. (b) Generalization across benchmarks. Comprehensiveness, depth, instruction following, and readability are measured following DeepResearch Bench (Du et al., 2025a). We report our model's performance for Deep xxxxx
> 
> # **1 Introduction**
> 
> Recent advances in deep research agents suggest that increasing reasoning depth and the number of tool calls can substantially improve task performance [\(Jin et al.,](#page-13-1) [2025;](#page-13-1) [Li et al.,](#page-14-0) [2025d,](#page-14-0)[e;](#page-14-1) [Qiu et al.,](#page-14-2) [2025;](#page-14-2) [Roucher et al.,](#page-14-3) [2025;](#page-14-3) [Sun](#page-14-4) [et al.,](#page-14-4) [2025;](#page-14-4) [Tang et al.,](#page-15-0) [2025b;](#page-15-0) [Wu et al.,](#page-15-1) [2025b;](#page-15-1) [Zhang et al.,](#page-15-2) [2025;](#page-15-2) [Zheng et al.,](#page-15-3) [2025;](#page-15-3) [Zhou et al.,](#page-15-4) [2023,](#page-15-4) [2024;](#page-15-5) [Zhu](#page-15-6) [et al.,](#page-15-6) [2025a,](#page-15-6)[b\)](#page-15-7). However, longer reasoning trajectories also lead to higher inference latency. Balancing long-horizon search performance and computational efficiency remains an open problem. [\(Chen et al.,](#page-13-2) [2025;](#page-13-2) [MiroMind Team,](#page-14-5) [2025;](#page-14-5) [Tang et al.,](#page-15-8) [2025a;](#page-15-8) [Tongyi DeepResearch Team,](#page-15-9) [2025\)](#page-15-9). To train agent models that can perform efficient long-horizon reasoning, a key question is: How can we design tasks that require efficient long-horizon reasoning, and construct suitable training trajectories to teach such behavior?
> 
> In addition, generalization across diverse task objectives and evaluation criteria remains challenging. Existing agentic search tasks can be roughly divided into two types. The first includes deterministic question-answering tasks with clear ground-truth answers, such as BrowseComp [\(Wei et al.,](#page-15-10) [2025\)](#page-15-10), GAIA [\(Mialon et al.,](#page-14-6) [2023\)](#page-14-6), and WebWalker [\(Wu et al.,](#page-15-11) [2025c\)](#page-15-11), where performance is mainly measured by accuracy. The second type focuses on open-ended research problems without a single correct answer, such as DeepResearch Bench [\(Du et al.,](#page-13-0) [2025a\)](#page-13-0) and DeepResearchGym [\(Coelho et al.,](#page-13-3) [2025\)](#page-13-3), where evaluation emphasizes information coverage, coherence, and synthesis quality. These two settings have very different optimization objectives. As a result, agents trained for one setting may struggle to generalize to the other, making it difficult to train a single agent that performs well across both.
> 
> In this work, we revisit long-horizon agentic search from the perspectives of efficiency and generalization. We argue that the main scalability bottleneck of existing deep research agents lies in their reliance on linear, sequential reasoning in search tasks. To address this, we propose an agentic framework that replaces sequential reasoning with parallel task decomposition and concurrent tool execution, together with structured context management for efficient long-horizon inference under constrained context budgets. At the data level, we introduce an automated pipeline that synthesizes representative multi-type search tasks, reducing redundant samples and improving generalization. Trained end-toend with supervised fine-tuning and reinforcement learning, our approach achieves state-of-the-art performance on BrowseComp (44.3), while substantially improving efficiency by reducing the number of reasoning steps by 78% and inference latency by up to 2.6×.
> 
> Our contributions are summarized as follows:
> 
> - **Parallel agentic workflow.** We propose a unified agentic framework that replaces sequential reasoning with parallel evidence acquisition, using plan-driven context management to achieve efficient long-horizon search under constrained context budgets.
> - **Generalized data construction.** We introduce an automated data pipeline that constructs representative multi-type search tasks across both deterministic and open-ended settings, supporting generalization in long-horizon agentic search.
> - **State-of-the-art performance.** Our method achieves state-of-the-art results on multiple deep search and deep research benchmarks, while substantially improving efficiency in terms of reasoning steps and inference latency.
> 
> # **2 Related Work**
> 
> *Agent Frameworks and Systems.* Agentic workflows augment large language models (LLMs) with planning, multi-step tool use, and iterative environment interaction, and have become the dominant paradigm for search-intensive tasks. A mainstream line of work relies on external orchestration, where a controller decomposes queries into subgoals, schedules web search, browsing, and code-execution tools, and aggregates intermediate findings through predefined procedures. This paradigm underlies recent commercial deep research systems that combine strong proprietary backbones with multi-step web exploration, plan refinement, and long-context memory to maintain intermediate evidence [\(Anthropic,](#page-13-4) [2025;](#page-13-4) [Google,](#page-13-5) [2024;](#page-13-5) [OpenAI,](#page-14-7) [2025;](#page-14-7) [Perplexity Team,](#page-14-8) [2025\)](#page-14-8). In parallel, structured agentic frameworks such as WebWeaver [\(Li et al.,](#page-14-9) [2025f\)](#page-14-9) and OAgents [\(Zhu et al.,](#page-15-6) [2025a\)](#page-15-6) adopt planner–researcher or planner–executor workflows to improve robustness in open-ended research and verification tasks. The open-source ecosystem further provides reusable orchestration templates, including plan-and-execute pipelines and hierarchical agent systems [\(LangChain,](#page-13-6) [2025;](#page-13-6) [SkyworkAI,](#page-14-10) [2025;](#page-14-10) [Together AI,](#page-15-12) [2025\)](#page-15-12), with MiroFlow consolidating these patterns into a benchmark-oriented research-agent framework [\(MiroMind AI Team,](#page-14-11) [2025\)](#page-14-11). Multi-agent workflows additionally introduce division of labor
> 
> through specialized roles and coordination cycles (Chen et al., 2023; Fourney et al., 2024; Hong et al., 2024; Hu et al., 2025; Li et al., 2023; Qian et al., 2023; Roucher et al., 2025). Despite architectural diversity, most existing agentic workflows implicitly scale performance by deepening sequential reasoning and expanding interaction horizons. As a consequence, this paradigm often leads to limited information efficiency, as substantial computation is devoted to prolonged model-side reasoning rather than effective external evidence acquisition.
> 
> Synthetic Data Pipelines. High-quality training data is critical for search agents, yet collecting multi-step, tool-interactive trajectories at scale remains costly. Recent synthetic data pipelines can be broadly categorized into two approaches. The first is graph-based generation, exemplified by WebSailor (Li et al., 2025a), which constructs knowledge graphs from seed entities using web tools and samples complex question—answer pairs or trajectories from subgraphs. The second approach follows an easy-to-hard expansion paradigm, where simple seed questions are progressively expanded into longer-horizon problems with predominantly tree-structured logic, as seen in WebShaper, ASearcher, and WebExplorer (Gao et al., 2025; Liu et al., 2025c; Tao et al., 2025a). Beyond web-centric pipelines, TaskCraft expands atomic tasks along both depth and width dimensions and applies incremental validation mechanisms, thereby improving data quality and controllability (Shi et al., 2025). Overall, existing synthetic pipelines highlight the effectiveness of tool-in-the-loop generation, while primarily emphasizing task difficulty or context length rather than explicitly shaping information-efficient search and verification behaviors. Besides, these pipelines are primarily designed around deterministic question answering or tightly constrained task structures, and provide limited support for synthesizing open-ended research tasks that require flexible information aggregation and cross-source validation.
> 
> ## <span id="page-2-1"></span>3 Parallel Agentic Workflow
> 
> Recent tool-augmented agents enable language models to interact with external tools for retrieval and reasoning (Li et al., 2025e; Xue et al., 2025; Yao et al., 2023). Building on this line of work, we design an efficient parallel agentic workflow inspired by Flash-Searcher (Qin et al., 2025), which explicitly supports concurrent execution and structured coordination across subtasks. As illustrated in Figure 2, the agent initializes a task plan, executes multiple subtasks in parallel, and periodically synchronizes intermediate results through plan refinement before producing the final answer.
> 
> <span id="page-2-0"></span>![[chen-22675-fig-003.jpeg]]
> 
> **Figure 2** Overview of our parallel agentic workflow design.
> 
> Initial Plan Construction. Given a composite search task, the agent first constructs an initial task plan  $G^0_{\text{plan}}$  by decomposing the problem into a set of interrelated yet partially independent <u>subtasks</u>. Each subtask corresponds to a concrete information-seeking or verification objective, such as retrieving facts, validating relations, or gathering evidence. The plan is generated prior to any tool execution and is designed to expose parallelizable execution paths early, enabling concurrent evidence acquisition and higher information density. Rather than relying on a single sequential reasoning chain,  $G^0_{\text{plan}}$  provides a structured starting point that supports parallel execution and subsequent refinement.
> 
> Parallel Execution and Tool Coordination. At each timestep, the system selects ready-to-execute subtasks from the pending set  $\mathcal{P}_t$  based on their readiness, while completed subtasks are tracked separately. These pending subtasks are processed concurrently, leveraging available tools or agent actions to gather information and execute reasoning tasks. By executing multiple pending subtasks in parallel, the system accelerates task completion and reduces sequential bottlenecks. The system aggregates observations from each parallel execution into a unified reasoning state:
> 
> $$s_{t+1} = \mathcal{F}(s_t, \{a_t^{(k)}\}_{k=1}^m, \{o_t^{(k)}\}_{k=1}^m), \tag{1}$$
> 
> where  $a_t^{(k)}$  and  $o_t^{(k)}$  represent the actions and observations of the k-th parallel execution, and  $s_t$  denotes the aggregated reasoning state at timestep t.
> 
> In practice, parallel execution is realized via a limited set of reusable external tools (Appendix A), including web search and page crawling, which are repeatedly invoked across pending subtasks to facilitate concurrent information acquisition and verification.
> 
> *Dynamic Plan Refinement.* To ensure the plan adapts to the ongoing execution, the task plan is periodically updated. Completed subtasks are removed, unresolved dependencies are rechecked, and new subtasks may be introduced. The task plan is refined based on the current execution state:
> 
> $$G_{\text{plan}}^{t+\Delta} = \mathcal{R}(G_{\text{plan}}^t, \mathcal{C}_t, \mathcal{P}_t, s_t),$$
> 
> where  $C_t$  represents the completed subtasks. This dynamic refinement ensures that the task adapts to progress and maintains efficiency.
> 
> #### Algorithm 1 Parallel agentic workflow.
> 
> ```
> Require: Composite task T
>   1: Construct initial task plan G_{\text{plan}}^0 \leftarrow \mathcal{D}(T)
>  2: Initialize reasoning state s_0, pending subtasks \mathcal{P}_0 \leftarrow G_{\text{plan}}^0, completed subtasks \mathcal{C}_0 \leftarrow \emptyset
>  3: while \mathcal{P}_t \neq \emptyset do
>           Select executable subtasks \mathcal{E}_t \subseteq \mathcal{P}_t
>  4:
>           Execute subtasks in \mathcal{E}_t in parallel using available tools
>  5:
>           Update reasoning state s_{t+1} = \mathcal{F}(s_t, \{a_t^{(k)}\}, \{o_t^{(k)}\}) Mark completed subtasks and update \mathcal{C}_{t+1}
>  6:
>  7:
>           Remove completed subtasks from \mathcal{P}_{t+1}
>  8:
>           if t \mod \Delta = 0 then
>  9:
>               Refine task plan using current execution state
> 10:
>               G_{\text{plan}}^{t+1} \leftarrow \mathcal{R}(G_{\text{plan}}^t, \mathcal{C}_{t+1}, \mathcal{P}_{t+1}, s_{t+1})
> 11:
> 12:
> 13: end while
> 14: Return final reasoning state s_T
> ```
> 
> <span id="page-3-0"></span>Algorithmic Outline. The workflow design is summarized in Algorithm 1. This method continuously refines the task plan while executing multiple subtasks in parallel, ensuring efficient task completion through dynamic updates and parallel execution.
> 
> #### 4 Data Construction
> 
> Existing agent data construction pipelines limit both generalization and efficiency. Many rely on static knowledge sources and focus on deterministic, entity-centric Deep Search tasks, providing weak coverage of open-ended Deep Research scenarios. Moreover, task difficulty is often scaled by increasing reasoning hops rather than improving information density, leading to redundant evidence and inefficient interaction traces.
> 
> To address these issues, we propose a **high-diversity**, **high-density** data synthesis pipeline. As illustrated in Fig. 3, our framework integrates raw corpus collection, graph construction, subgraph extraction, and QA generation with verification, enabling dense and correlated evidence acquisition for both deterministic and open-ended research tasks.
> 
> <span id="page-4-0"></span>![[chen-22675-fig-004.jpeg]]
> 
> Figure 3 Overview of the data construction pipeline.
> 
> ## 4.1 Deep Search Data
> 
> Raw Corpus Collection. To support cross-domain generalization and multi-hop reasoning, we construct the raw corpus by leveraging trajectories from TaskCraft corpus (Shi et al., 2025). These trajectories contain rich collections of real-world URLs spanning diverse domains, including art, sports, history, government, economics, politics, music, geography, movies, computer science, physics, and chemistry.
> 
> Crucially, URLs within each trajectory are not independent: they are connected through explicit information-seeking paths, where later queries and sources build upon evidence gathered from earlier ones. This structure naturally induces multi-hop relationships across documents, making the collected corpus well suited for graph-based task construction. We extract and normalize the content from these URLs using document parsing tool Jina (Jina, 2025), yielding a large-scale, high-diversity raw corpus that preserves both domain coverage and inter-document relational structure.
> 
> Graph Network Construction. Based on the initial corpus, we develop an efficient pipeline for generating sophisticated graph networks. First, we leverage LightRAG (Guo et al., 2024) to instantiate a knowledge-driven graph network. This process involves partitioning the curated text crops into multiple chunks, from which an LLM extracts entities and their respective attributes. By integrating an embedding-plus-reranker retrieval mechanism, we recall pertinent chunks, enabling the LLM to synthesize detailed node descriptions and delineate complex inter-node relationships, ultimately culminating in a highly intricate graph network.
> 
> Subgraph Extraction. Given the constructed knowledge graph, we extract task-specific subgraphs using a controlled random-walk strategy. For each task, we sample a target entity as the ground-truth answer and perform a breadth-first search (BFS) up to N hops to collect its surrounding neighborhood. The resulting subgraph defines the supporting evidence structure required to infer the answer, where multi-hop nodes serve as question conditions with varying degrees of indirection. By adjusting the hop depth and branching factor, we flexibly control task difficulty while preserving semantic coherence and factual correctness. This construction yields compact, information-dense task skeletons that require integrating multiple related evidence sources rather than relying on single-hop retrieval.
> 
> To ensure high-quality and non-degenerate task structures, we adopt the following design principles:
> 
> - **Rich topological structures.** We prioritize subgraphs in which two (N+1)-hop nodes are interrelated while sharing a common N-hop parent, inducing cyclic dependencies that require cross-validation of multiple relationships.
> - **Controlled walk width and depth.** We explicitly bound the depth and branching factor to keep task difficulty scalable and avoid trivial shortcuts or excessively long reasoning chains.
> 
> *Hierarchical Question Construction and Verification.* Given a task-specific subgraph with a fixed target answer, we construct questions through a hierarchical synthesis process. Starting from the outermost N-hop frontier, we iteratively aggregate information from (i+1)-hop nodes to form sub-questions about i-hop entities. Each aggregation step yields a valid intermediate question, and progressively merging all layers produces a final question about the target entity that requires the maximum hop depth and reasoning difficulty.
> 
> When multiple (i+1)-hop nodes exhibit semantic relationships, we explicitly encode these interdependencies as verifiable conditions, requiring agents to cross-validate parallel evidence paths rather than rely on linear reasoning. To prevent information leakage, we apply an LLM-based verification step after each synthesis iteration; if the answer can be inferred prematurely, the question is restructured or relevant information is obfuscated. This process repeats until the desired difficulty is achieved or a maximum of five iterations is reached.
> 
> The complete designs of the prompts for entity information extraction, entity evaluation, entity question generation and obfuscation, and verification are described in [Section B.1.](#page-21-0)
> 
> ## **4.2 Deep Research Data**
> 
> *Research Question Construction.* Deep research tasks are synthesized entirely within our unified data construction pipeline, without relying on externally curated queries. Given a task-specific subgraph with a fixed target entity and its multi-hop supporting structure, we formulate open-ended research questions that require integrating evidence across the entire subgraph. These questions are designed to elicit report-style answers involving explanation, comparison, and synthesis across multiple sources, rather than single factual outputs. This construction explicitly encourages long-horizon planning, information aggregation, and cross-source reasoning.
> 
> *Trajectory Generation and Quality Filtering.* For each open-ended research question, we generate multiple candidate trajectories using our parallel agentic workflow and apply a two-stage filtering process to ensure high-quality supervision. First, trajectories are subjected to rule-based hard rejection to remove structurally invalid or overly shallow solutions, enforcing basic requirements on completeness, context validity, reasoning depth, tool usage, and format consistency. Second, trajectories that pass these checks are then evaluated by an LLM-as-a-Judge, which assesses higher-level semantic quality, including comprehensiveness, insight/depth, instruction following, and readability. Only trajectories that satisfy both structural constraints and semantic criteria are retained as reference outputs, yielding scalable and reliable training data for open-ended deep research tasks.
> 
> The complete designs of the prompts for open-ended question construction and quality filtering are illustrated in [Section B.2.](#page-22-0)
> 
> # **5 Training Recipe**
> 
> ## **5.1 Post-training Supervised Fine-tuning.**
> 
> We perform supervised fine-tuning (SFT) to initialize the agent with stable and efficient search behaviors before reinforcement learning.
> 
> *Task Composition.* The SFT dataset consists of two task categories: Deep Search and Deep Research, which differ in supervision form while sharing the same underlying subgraph-based construction.
> 
> • **Deep Search.** Tasks are instantiated from task-specific subgraphs with hop depth ranging from 2 to 5. For each subgraph, all hierarchical question variants constructed during iterative aggregation are retained, yielding multiple questions that share the same target entity as the ground-truth answer. To prevent over-representation of frequent answers, we apply an answer frequency threshold and discard tasks whose target entities appear excessively often.
> 
> • **Deep Research.** For each subgraph, we construct an open-ended research question centered on the target entity and its multi-hop supporting structure. Questions are formulated to encourage broad exploration and synthesis over the entire subgraph, rather than single-answer retrieval, ensuring sufficient topical richness and variability.
> 
> *Trajectory Construction and Curation.* Training trajectories are generated using the agentic workflow described in Section [3.](#page-2-1) For Deep Search tasks, supervision is obtained by distilling trajectories generated by DeepSeek-V3.2 [\(Liu et al.,](#page-14-16) [2025a\)](#page-14-16), while Deep Research trajectories are distilled from GPT-5 [\(OpenAI,](#page-14-17) [2025\)](#page-14-17), reflecting its stronger long-form synthesis capabilities.
> 
> To ensure high-quality supervision, we apply the following curation criteria:
> 
> - **Context length constraint.** The total trajectory length is capped at 64K tokens to reduce redundant interactions and noisy supervision.
> - **Interaction efficiency constraint.** The average number of tool calls per step must be no less than 3, encouraging active information acquisition.
> - **Trajectory efficiency optimization.** For tasks with multiple successful trajectories, we retain only those that are correct and shortest in interaction length.
> 
> ## **5.2 Reinforcement Learning.**
> 
> We adopt a slightly modified version of the REINFORCE Leave-One-Out (RLOO) algorithm [\(Ahmadian et al.,](#page-13-16) [2024\)](#page-13-16) as our reinforcement learning method. Compared to GRPO, RLOO provides an unbiased advantage estimator. Our modifications are as follows. First, following the implementation in DAPO [\(Yu et al.,](#page-15-16) [2025\)](#page-15-16), we employ a token-level loss function. Second, to mitigate the training–inference mismatch arising from discrepancies between the inference engine and the training framework in log-probability computation, we apply sequence-level importance sampling for rollout correction [\(Liu et al.,](#page-14-18) [2025b\)](#page-14-18). Third, to ensure trajectory quality, we filter out certain negative trajectories so that they do not participate in advantage estimation or gradient updates. These negative trajectories include (i) failures caused by environmental issues such as connection timeouts or server errors, and (ii) responses that are excessively long or reach the maximum number of turns. This filtering strategy prevents the model from learning spurious behaviors induced by environmental instability and effectively stabilizes training.
> 
> During the RL stage, we optimize trajectories using an outcome-based reward. An LLM-as-a-judge evaluates whether the final answer is correct, assigning a reward of 1 for correct answers and 0 otherwise. Notably, if a tool call violates the required format, generation is immediately terminated and a reward of 0 is assigned, thereby explicitly encouraging correct tool usage.
> 
> # **6 Experiments**
> 
> ## **6.1 Experimental Setup**
> 
> *Baselines.* For comparison, we conducted a comprehensive evaluation of our SMTL model against three categories of systems: (1) foundation models with tools, including closed-source models such as Claude-4.5-Sonnet [Anthropic](#page-13-17) [\(2025\)](#page-13-17), OpenAI-GPT-5 [OpenAI](#page-14-17) [\(2025\)](#page-14-17), Gemini-2.5-Pro [Comanici et al.](#page-13-18) [\(2025\)](#page-13-18), and open-source models GLM-4.5 [Zeng](#page-15-17) [et al.](#page-15-17) [\(2025\)](#page-15-17), Minimax-M2 [MiniMax](#page-14-19) [\(2025\)](#page-14-19), DeepSeek-V3.2 [Liu et al.](#page-14-16) [\(2025a\)](#page-14-16), Kimi-K2-0905 [Team et al.](#page-15-18) [\(2025\)](#page-15-18); (2) deep research system, including OpenAI DeepResearch [OpenAI](#page-14-7) [\(2025\)](#page-14-7), Gemini DeepResearch [Google](#page-13-19) [\(2025\)](#page-13-19), Perplexity Deep Research [Perplexity](#page-14-20) [\(2025\)](#page-14-20), Kimi-Researcher [Moonshot](#page-14-21) [\(2025\)](#page-14-21), OAgents (Claude-3-7) [Zhu et al.](#page-15-6) [\(2025a\)](#page-15-6), MiroFlow (GPT-5) [MiroMind AI Team](#page-14-11) [\(2025\)](#page-14-11); and (3) open-source agentic models, including WebSailor-32B [Li et al.](#page-13-20) [\(2025b\)](#page-13-20), WebDancer-QwQ [Wu et al.](#page-15-19) [\(2025a\)](#page-15-19), WebShaper-32B [Tao et al.](#page-15-20) [\(2025b\)](#page-15-20), DeepMiner-32B-RL [Tang](#page-15-8) [et al.](#page-15-8) [\(2025a\)](#page-15-8), AFM-32B-RL [Li et al.](#page-13-21) [\(2025c\)](#page-13-21), Tongyi DeepResearch-30B [Tongyi DeepResearch Team](#page-15-9) [\(2025\)](#page-15-9), and MiroThinker-v1.0-30B [MiroMind Team](#page-14-5) [\(2025\)](#page-14-5).
> 
> *Evaluation Benchmarks.* We evaluate all models' performance on a broad set of agentic benchmarks covering both deep search and deep research scenarios. Specifically, the deep search benchmarks include BrowseComp [Wei et al.](#page-15-10) [\(2025\)](#page-15-10), GAIA [Mialon et al.](#page-14-6) [\(2023\)](#page-14-6), XBench-DeepSearch [Xbench-Team](#page-15-21) [\(2025\)](#page-15-21), WebWalkerQA [Wu et al.](#page-15-11) [\(2025c\)](#page-15-11), FRAMES [Krishna et al.](#page-13-22) [\(2025\)](#page-13-22), and SEAL-0 [Pham et al.](#page-14-22) [\(2025\)](#page-14-22). For deep research evaluation, we use Deep Research
> 
> Bench RACE [Du et al.](#page-13-23) [\(2025b\)](#page-13-23), which evaluates long-form, open-ended research reports and reports both an overall score and four fine-grained criteria: Comprehensiveness, Insight, Instruction Following, and Readability.
> 
> *Metrics.* We use LLM-as-judge approach for evaluation. For deep search tasks, we adopt the pass@1 metric with a specific judge prompt. For Deep Research Bench RACE, which assesses report quality across four criteria: Comprehensiveness (coverage of key aspects), Insight/Depth (analytical novelty), Instruction-Following (adherence to query constraints), and Readability (clarity and coherence), we use another judge prompt. See the two judge prompts in [D.3.](#page-28-0)
> 
> *Implementation Details.* We conduct experiments using the backbone model: Qwen3-30B-A3B-Instruct-2507. During supervised fine-tuning, we train the models for 3.5 epochs with a batch size of 128, using the AdamW optimizer and a cosine decay learning rate schedule with an initial learning rate of 1.4 × 10<sup>−</sup><sup>5</sup> . The maximum sequence length is set to 65,536 tokens to support long-horizon trajectories. All models are trained under the same agentic workflow and data settings described in earlier sections. In the RL stage, the learning rate is set to 1× 10<sup>−</sup><sup>6</sup> with a batch size of 32. For each question, 8 on-policy rollouts are generated, with a maximum sequence length of 128k tokens, up to 120 interaction turns, and training is performed for 60 steps. During inference, we use vLLM, with a context window of 128K tokens. Unless otherwise specified, all experiments are conducted with a maximum of 100 interaction steps, a plan refinement interval of N=5 interaction steps.
> 
> *Context Management during Inference Stage.* Long-horizon tasks (e.g., BrowseComp) often exceed the effective context capacity of a vanilla agent under a 128K window, and this issue is amplified in SMTL because each interaction step produces more tool observations, reducing the number of steps that can be accommodated before hitting the context limit. To improve context efficiency, SMTL couples periodic plan refinement with an overflow-triggered compression scheme: the agent refines the task plan every N=5 steps by default, and when the accumulated history reaches the 128K context budget without a confirmed answer, it performs an additional forced plan refinement using the current history, then drops all pre-plan context and continues execution from the refreshed plan. This plan-centric reset preserves the latest execution state and subtask structure, keeping inference behavior aligned with training-time plan refinement. As a result, SMTL supports longer effective trajectories under a fixed context budget without sacrificing structured task context. To align with the interaction budgets commonly used by baselines, we further evaluate SMTL with maximum step limits of 100 and 300, referred to as SMTL-100 and SMTL-300, respectively. For Deep Research Bench, we report results under the 100-step setting, as open-ended research tasks typically converge within tens of interactions rather than exhausting the maximum interaction budget.
> 
> ## **6.2 Main Results**
> 
> *SMTL exhibits consistent Pareto dominance across heterogeneous benchmarks.* As shown in Table [1,](#page-8-0) SMTL-30B demonstrates strong performance across Deep Search benchmarks under different interaction budgets. With a moderate budget (SMTL-100), the model already achieves state-of-the-art performance among 30B-scale open-source agentic models on BrowseComp (43.6%), slightly surpassing Tongyi-DeepResearch-30B (43.4%) and clearly outperforming MiroThinker-v1.0-30B (41.2%). It also reaches 78.0% on XBench-DeepSearch and 74.9% on WebWalker-QA. When the budget is increased to 300 steps, performance further improves, most notably on the long-horizon benchmark BrowseComp, where accuracy rises from 43.6% to **48.6%** (+5.0), substantially widening the gap over both Tongyi and MiroThinker. In contrast, gains on shorter-horizon tasks such as GAIA (74.8% → **75.7%**) and WebWalker (74.9% → **76.5%**) are comparatively modest, indicating that additional interaction budget primarily benefits deeper multi-step evidence aggregation. Moreover, as shown in Figure [1a,](#page-0-0) across interaction budgets from 50 to 300 steps on BrowseComp, SMTL consistently lies on the Pareto frontier of accuracy versus trajectory length, while mainstream baselines fall below this curve, confirming its superior efficiency-aware scaling on long-horizon search.
> 
> Beyond deterministic deep search, SMTL further generalizes to open-ended deep research evaluation. On DeepResearch Bench RACE, SMTL-100 achieves an overall score of **45.9%**, with strong and balanced performance across Comprehensiveness (42.1%), Insight/Depth (45.6%), Instruction Following (49.6%), and Readability (45.5%). This surpasses representative open-source agentic baselines such as WebSailor-32B (32.4%), WebDancer-QwQ (35.9%), WebShaper-32B (34.9%), and AFM-32B-RL (35.8%), and slightly outperforms Tongyi-DeepResearch-30B (45.7%) and Kimi-Researcher (44.6%), establishing strong competitiveness among 30B-scale systems. This demonstrates that the same parallel search framework transfers from accuracy-driven benchmarks to long-form research synthesis
> 
> <span id="page-8-0"></span>**Table 1** Main results on Deep Search and Deep Research benchmarks. For Deep Search benchmarks, BC: BrowseComp; Xbench-DS: Xbench-DeepSearch; WW: WebWalker-QA; DR-Gym: DeepResearch Gym; DR-Bench: DeepResearch Bench. For Deep Research Bench, Comp., Depth, Inst., and Read. denote Comprehensiveness, Insight/Depth, Instruction Following, and Readability, respectively. We report pass@1 for our models. Results marked with <sup>∗</sup> are obtained by our own evaluation, while results marked with <sup>+</sup> are taken from prior work [\(Yao et al.,](#page-15-22) [2026\)](#page-15-22).
> 
> | Model                        | Deep Search |      |           |      |        |        | Deep Research Bench RACE |       |       |       |       |  |
> |------------------------------|-------------|------|-----------|------|--------|--------|--------------------------|-------|-------|-------|-------|--|
> |                              | BC          | GAIA | Xbench-DS | WW   | FRAMES | SEAL-0 | Overall.                 | Comp. | Depth | Inst. | Read. |  |
> | Foundation Models with Tools |             |      |           |      |        |        |                          |       |       |       |       |  |
> | GLM-4.5                      | 26.4        | 66.0 | 70.0      | 65.6 | 78.9   | 36.0   | –                        | –     | –     | –     | –     |  |
> | Minimax-M2                   | 44.0        | 75.7 | 72.0      | 64.5 | –      | –      | 46.1                     | 45.2  | 44.6  | 49.0  | 44.7  |  |
> | DeepSeek-V3.2                | 40.1        | 63.5 | 71.0      | –    | 80.2   | 38.5   | –                        | –     | –     | –     | –     |  |
> | Kimi-K2-0905                 | 14.1        | 60.2 | 61.0      | 63.0 | 58.1   | 25.2   | 44.5                     | 42.8  | 39.7  | 50.8  | 46.0  |  |
> | Claude-4.5-Sonnet            | 19.6        | 71.2 | 66.0      | –    | 85.0   | 53.4   | 39.9                     | –     | –     | –     | –     |  |
> | GPT-5                        | 54.9        | 59.4 | –         | 73.0 | 90.0   | 51.4   | 46.8                     | 45.4  | 44.5  | 50.3  | 47.5  |  |
> | Gemini-2.5-Pro               | -           | 60.2 | 56.0      | –    | –      | 19.8   | 35.1                     | 34.1  | 29.8  | 41.7  | 37.2  |  |
> | Deep Research System         |             |      |           |      |        |        |                          |       |       |       |       |  |
> | OpenAI DeepResearch          | 51.5        | 67.4 | 26.6      | –    | –      | –      | 47.0+                    | 46.9+ | 45.3+ | 49.3+ | 47.1+ |  |
> | Gemini DeepResearch          | 59.2        | –    | 50.0      | –    | –      | –      | 48.9+                    | 48.5+ | 48.5+ | 49.2+ | 49.4+ |  |
> | Perplexity Deep Research     | 22.0        | –    | –         | 67.0 | 83.0   | 38.7   | 42.3+                    | 40.7+ | 39.4+ | 46.4+ | 44.3+ |  |
> | Kimi-Researcher              | 26.9        | –    | 69.0      | –    | 78.8   | 36.0   | 44.6                     | 45.0  | 42.0  | 47.1  | 45.6  |  |
> | OAgents (Claude-3-7)         | 22.2        | 66.7 | 54.5      | 53.0 | –      | –      | 50.8+                    | 50.4+ | 51.2+ | 50.3+ | 49.4+ |  |
> | MiroFlow (GPT-5)             | 33.2        | 82.4 | 72.0      | 52.6 | –      | –      | 44.9                     | 44.6  | 49.3  | 45.6  | 46.1  |  |
> | Open-source Agentic Model    |             |      |           |      |        |        |                          |       |       |       |       |  |
> | WebSailor-32B                | 10.5        | 53.2 | 53.3      | 60.5 | 69.8   | 16.2   | 32.4                     | 28.6  | 22.7  | 43.7  | 37.5  |  |
> | WebDancer-QwQ                | 3.8         | 51.5 | 40.0      | 47.9 | –      | 20.7   | 35.9∗                    | 33.0∗ | 28.5∗ | 44.8∗ | 40.8∗ |  |
> | WebShaper-32B                | 9.0         | 52.4 | 54.6      | 51.4 | –      | –      | 34.9                     | 31.6  | 26.2  | 44.8  | 40.4  |  |
> | DeepMiner-32B-RL             | 33.5        | 58.7 | 62.0      | –    | –      | –      | –                        | –     | –     | –     | –     |  |
> | AFM-32B-RL                   | 11.1        | 55.3 | 52.0      | 63.0 | –      | –      | 35.8∗                    | 32.7∗ | 30.2∗ | 38.5∗ | 40.9∗ |  |
> | NestBrowse-30B-A3B           | 31.6        | 75.7 | 75.0      | –    | –      | –      | –                        | –     | –     | –     | –     |  |
> | Tongyi-DeepResearch-30B      | 43.4        | 70.9 | 75.0      | 72.2 | 90.6   | –      | 45.7+                    | 44.7+ | 44.2+ | 48.8+ | 44.2+ |  |
> | MiroThinker-v1.0-30B         | 41.2        | 73.5 | 70.6      | 61.0 | 85.4   | 46.8   | –                        | –     | –     | –     | –     |  |
> | SMTL-100                     | 43.6        | 74.8 | 80.0      | 74.9 | 84.3   | 50.5   | 45.9                     | 42.1  | 45.6  | 49.6  | 45.5  |  |
> | SMTL-300                     | 48.6        | 75.7 | 82.0      | 76.5 | 85.1   | 51.4   | -                        | -     | -     | -     |       |  |
> 
> without task-specific modification. Furthermore, as illustrated in Figure [1b,](#page-0-0) SMTL consistently achieves leading average performance across heterogeneous benchmarks, confirming that its gains are not confined to a single task but reflect robust generalization across diverse objectives and evaluation criteria.
> 
> *Why SMTL is more efficient?* We analyze the efficiency advantage of SMTL through a qualitative case study comparing SMTL-30B with a representative deep-search baseline, MiroThinker-v1.0-30B, on a BrowseComp task. As illustrated in Figure [5,](#page-23-0) SMTL localizes the key entity within 8 assistant turns, whereas MiroThinker-v1.0 requires 16 turns to reach the same evidence.
> 
> This difference arises from fundamentally different search organization strategies. SMTL decomposes the task into multiple hypothesized subtasks and explores them in parallel, allowing the agent to rapidly surface high-signal evidence and to periodically re-plan subtasks based on intermediate observations. As a result, SMTL quickly converges on the correct search direction and allocates subsequent interactions to evidence verification. In contrast, MiroThinker-v1.0 follows a strictly sequential interaction pattern, in which only a single tool call is permitted per round. Information gathering therefore proceeds incrementally, requiring repeated query reformulation and delaying the discovery of key evidence.
> 
> This case study demonstrates that SMTL's efficiency gains do not stem from deeper per-step reasoning, but from parallel subtask exploration and staged re-planning. By reorganizing search execution rather than extending reasoning depth, SMTL substantially reduces the number of interaction rounds required to localize critical information and complete the task.
> 
> # **7 Analysis**
> 
> # **7.1 Efficiency Evaluation**
> 
> <span id="page-9-0"></span>**Table 2 Interaction efficiency on BrowseComp.** Average number of assistant steps, average tool calls per step, and task accuracy.
> 
> | Model                   | Steps | Avg. Tool Calls | Acc. |
> |-------------------------|-------|-----------------|------|
> | Tongyi-DeepResearch-30B | 75.2  | 1.0             | 43.4 |
> | MiroThinker-v1.0-30B    | 206.0 | 1.0             | 41.2 |
> | SMTL-100                | 60.4  | 3.5             | 44.6 |
> | SMTL-300                | 150.7 | 3.7             | 48.6 |
> 
> We evaluate the efficiency of SMTL from the perspective of interaction complexity, measured by the number of assistant steps required to solve a task. As shown in Table [2,](#page-9-0) SMTL achieves a more favorable balance between interaction cost and task performance compared to representative baselines on BrowseComp. SMTL-100 attains 44.6% accuracy with an average of 60.4 assistant steps, slightly outperforming Tongyi-DeepResearch-30B (43.4%) while requiring fewer steps (75.2). The contrast with MiroThinker-v1.0-30B is even more pronounced: MiroThinker requires 206.0 steps to reach 41.2% accuracy, whereas SMTL-100 achieves substantially higher accuracy with less than one-third of the interaction cost.
> 
> This efficiency is closely related to SMTL's parallel execution mechanism. Unlike sequential systems that invoke a single tool per round, SMTL performs an average of 3.5 tool calls per step, enabling concurrent evidence acquisition across subtasks. By aggregating more information within each interaction round, SMTL increases information density per step and reduces redundant query reformulation, resulting in shorter yet more effective trajectories.
> 
> Increasing the interaction budget to SMTL-300 further improves accuracy to 48.6%, with 150.7 average steps. While the trajectory length grows as expected, the additional steps translate into measurable performance gains rather than inefficient looping. Figure [1a](#page-0-0) further illustrates that across different interaction budgets, SMTL consistently lies on the Pareto frontier in the accuracy–step plane, confirming that its performance improvements are achieved with well-controlled interaction complexity.
> 
> ## **7.2 Ablations on Maximum Interaction Steps.**
> 
> We conduct ablation studies to analyze how the maximum interaction budget (max steps) affects task outcomes and trajectory behavior in long-horizon agentic search. Specifically, we vary the maximum number of interaction steps from 50 to 300 on BrowseComp and report four statistics in Figure [4a:](#page-10-0) overall average steps, overall median steps, the median steps of successful cases, and the median steps of failed cases.
> 
> Several clear patterns emerge. First, the median step count of successful cases does not exhibit a noticeable increasing trend as interaction steps grows. Most successful trajectories converge before reaching the interaction limit, suggesting that once a correct reasoning path is identified, additional budget provides limited benefit for these cases. In contrast, the median step count of failed cases closely follows the y = x trend, indicating that the majority of failed trajectories terminate exactly at the maximum allowed step. This implies that many failures are due to exhausting the interaction budget rather than prematurely outputting incorrect answers. Consequently, the increase in overall average steps is primarily driven by the upward shift of failed cases, as more trajectories extend to the new budget ceiling before terminating. This observation suggests that the model is actively attempting to explore alternative reasoning paths when facing difficulty, rather than misunderstanding the task or exhibiting overconfidence through early answer generation.
> 
> We further analyze why increasing max interaction steps leads to performance gains. Under smaller budgets, a substantial portion of hard cases fail simply because SMTL cannot identify a valid reasoning path within the limited number of tool interactions. When the interaction budget is enlarged, SMTL is afforded additional opportunities to explore different evidence chains. Combined with periodic plan refinement, this extended budget enables our model to correct suboptimal
> 
> <span id="page-10-0"></span>![[chen-22675-fig-005.jpeg]]
> 
> Figure 4 Results of context management (CM) under different observation horizons K with interaction budgets (IB) of 80 and 160 steps on the BC benchmark.
> 
> search directions and progressively reorient toward promising subtasks. As a result, increasing max steps improves success rates primarily by alleviating search budget constraints in genuinely difficult cases, rather than by compensating for systematic reasoning errors.
> 
> ## 7.3 Ablations on Retrieval Top-k.
> 
> We further investigate how the retrieval width of the web search tool affects performance by varying the parameter top-k, which controls the number of URLs returned for each query. We evaluate both SMTL-100 and SMTL-300 on BrowseComp under different top-k settings.
> 
> As shown in Figure 4b, increasing top-k consistently improves task performance. When top-k increases from 4 to 8, both SMTL-100 and SMTL-300 exhibit substantial gains (e.g., SMTL-300 improves from 43.8 to 47.0, while SMTL-100 increases from 36.6 to above 41.8). This jump indicates that a narrow retrieval window significantly constrains evidence coverage, limiting the SMTL's ability to identify relevant information within a fixed interaction budget. When top-k further increases from 8 to 20, performance continues to improve, albeit at a slower rate and gradually converges. This suggests diminishing returns: once the most informative candidates are included, additional results contribute marginal gains but still enhance robustness by reducing the risk of missing critical evidence.
> 
> Overall, the results align with our design intuition that improving search breadth can be a powerful scaling dimension for long-horizon agentic search. Under a fixed number of interaction steps, increasing top-k effectively packs more candidate evidence into each search action, raising the information density per step. Rather than extending reasoning depth, SMTL benefits more from broader evidence acquisition within each interaction, demonstrating that expanding retrieval breadth is a more efficient scaling axis for long-horizon search than merely increasing reasoning length.
> 
> ## 8 Conclusion
> 
> In this work, we revisit long-horizon agentic search from the perspectives of efficiency and generalization. We propose Search More, Think Less (SMTL), a unified agentic framework that replaces sequential reasoning with parallel evidence acquisition, enabling efficient long-horizon inference under constrained interaction budgets. SMTL integrates an efficient agentic workflow with structured context management and an automated data synthesis pipeline that supports both deterministic question answering and open-ended research tasks. Trained end-to-end with supervised fine-tuning and reinforcement learning, SMTL achieves state-of-the-art or competitive performance across a wide range of deep
> 
> search and deep research benchmarks, while substantially reducing reasoning steps and inference latency. Our results demonstrate that prioritizing efficient, search-centric scaling over ever-deeper reasoning provides a practical and generalizable foundation for future deep research agents, and we hope this work encourages further exploration of efficiency-oriented agentic designs.
> 
> # **9 Contributions**
> 
> ## **Core Contributors**
> 
> - Qianben Chen Tianrui Qin
> - Chengjun Yu Shu Xu
> - Jiaqi Wu
> 
> #### **Contributors**
> 
> - Jiayu Zhang Xinpeng Liu
> - Piaohong Wang Dingfeng Shi
> - Yuqing Wang Maojia Song
> - Tianyu Zheng Ge Zhang
> 
> #### **Corresponding Author**
> 
> • Wangchunshu Zhou
> 
> - King Zhu Qiexiang Wang
> - Xin Gui Jingyi Cao
> - He Zhu Tiannan Wang
> - Jian Yang Jiaheng Liu
> - Minghao Liu Yuchen Eleanor Jiang
> 
> # **References**
> 
> - <span id="page-13-16"></span>Ahmadian, A., Cremer, C., Gallé, M., Fadaee, M., Kreutzer, J., Pietquin, O., Üstün, A., and Hooker, S. (2024). Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms.
> - <span id="page-13-4"></span>Anthropic (2025). Claude takes research to new places. <https://www.anthropic.com/news/research>.
> - <span id="page-13-17"></span>Anthropic (2025). Introducing claude sonnet 4.5. <https://www.anthropic.com/news/claude-sonnet-4-5>.
> - <span id="page-13-2"></span>Chen, A., Li, A., Gong, B., Jiang, B., Fei, B., Yang, B., Shan, B., Yu, C., Wang, C., Zhu, C., et al. (2025). Minimax-m1: Scaling test-time compute efficiently with lightning attention. arXiv preprint arXiv:2506.13585.
> - <span id="page-13-7"></span>Chen, W., Su, Y., Zuo, J., Yang, C., Yuan, C., Qian, C., Chan, C.-M., Qin, Y., Lu, Y., Xie, R., et al. (2023). Agentverse: Facilitating multi-agent collaboration and exploring emergent behaviors in agents. arXiv preprint arXiv:2308.10848, 2(4):6.
> - <span id="page-13-3"></span>Coelho, J., Ning, J., He, J., Mao, K., Paladugu, A., Setlur, P., Jin, J., Callan, J., Magalhães, J., Martins, B., et al. (2025). Deepresearchgym: A free, transparent, and reproducible evaluation sandbox for deep research. arXiv preprint arXiv:2505.19253.
> - <span id="page-13-18"></span>Comanici, G., Bieber, E., Schaekermann, M., et al. (2025). Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities.
> - <span id="page-13-0"></span>Du, M., Xu, B., Zhu, C., Wang, X., and Mao, Z. (2025a). Deepresearch bench: A comprehensive benchmark for deep research agents. arXiv preprint arXiv:2506.11763.
> - <span id="page-13-23"></span>Du, M., Xu, B., Zhu, C., Wang, X., and Mao, Z. (2025b). Deepresearch bench: A comprehensive benchmark for deep research agents.
> - <span id="page-13-8"></span>Fourney, A., Bansal, G., Mozannar, H., Tan, C., Salinas, E., Niedtner, F., Proebsting, G., Bassman, G., Gerrits, J., Alber, J., et al. (2024). Magentic-one: A generalist multi-agent system for solving complex tasks. arXiv preprint arXiv:2411.04468.
> - <span id="page-13-13"></span>Gao, J., Fu, W., Xie, M., Xu, S., He, C., Mei, Z., Zhu, B., and Wu, Y. (2025). Beyond ten turns: Unlocking long-horizon agentic search with large-scale asynchronous rl. arXiv preprint arXiv:2508.07976.
> - <span id="page-13-5"></span>Google (2024). Gemini deep research: Your personal research assistant. <https://gemini.google/overview/deep-research/>.
> - <span id="page-13-19"></span>Google (2025). Gemini deep research. <https://gemini.google/us/overview/deep-research/?hl=en>.
> - <span id="page-13-15"></span>Guo, Z., Xia, L., Yu, Y., Ao, T., and Huang, C. (2024). Lightrag: Simple and fast retrieval-augmented generation. arXiv preprint arXiv:2410.05779.
> - <span id="page-13-9"></span>Hong, S., Zhuge, M., Chen, J., Zheng, X., Cheng, Y., Zhang, C., Wang, J., Wang, Z., Yau, S. K. S., Lin, Z., et al. (2024). Metagpt: Meta programming for a multi-agent collaborative framework. In International Conference on Learning Representations, ICLR.
> - <span id="page-13-10"></span>Hu, M., Zhou, Y., Fan, W., Nie, Y., Xia, B., Sun, T., Ye, Z., Jin, Z., Li, Y., Chen, Q., Zhang, Z., Wang, Y., Ye, Q., Ghanem, B., Luo, P., and Li, G. (2025). Owl: Optimized workforce learning for general multi-agent assistance in real-world task automation.
> - <span id="page-13-1"></span>Jin, B., Zeng, H., Yue, Z., Yoon, J., Arik, S., Wang, D., Zamani, H., and Han, J. (2025). Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516.
> - <span id="page-13-14"></span>Jina, I. (2025). Jina reader.
> - <span id="page-13-22"></span>Krishna, S., Krishna, K., Mohananey, A., Schwarcz, S., Stambler, A., Upadhyay, S., and Faruqui, M. (2025). Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. In Chiruzzo, L., Ritter, A., and Wang, L., editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 4745–4759, Albuquerque, New Mexico. Association for Computational Linguistics.
> - <span id="page-13-6"></span>LangChain (2025). Open deep research. [https://github.com/langchain-ai/open\\_deep\\_research](https://github.com/langchain-ai/open_deep_research).
> - <span id="page-13-11"></span>Li, G., Hammoud, H. A. A. K., Itani, H., Khizbullin, D., and Ghanem, B. (2023). Camel: Communicative agents for "mind" exploration of large language model society. In Thirty-seventh Conference on Neural Information Processing Systems.
> - <span id="page-13-12"></span>Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., et al. (2025a). Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592.
> - <span id="page-13-20"></span>Li, K., Zhang, Z., Yin, H., Zhang, L., Ou, L., Wu, J., Yin, W., Li, B., Tao, Z., Wang, X., Shen, W., Zhang, J., Zhang, D., Wu, X., Jiang, Y., Yan, M., Xie, P., Huang, F., and Zhou, J. (2025b). Websailor: Navigating super-human reasoning for web agent.
> - <span id="page-13-21"></span>Li, W., Lin, J., Jiang, Z., Cao, J., Liu, X., Zhang, J., Huang, Z., Chen, Q., Sun, W., Wang, Q., Lu, H., Qin, T., Zhu, C., Yao, Y., Fan, S., Li, X., Wang, T., Liu, P., Zhu, K., Zhu, H., Shi, D., Wang, P., Guan, Y., Tang, X., Liu, M., Jiang, Y. E., Yang, J., Liu, J., Zhang, G., and Zhou, W. (2025c). Chain-of-agents: End-to-end agent foundation models via multi-agent distillation and agentic rl.
> 
> - <span id="page-14-0"></span>Li, X., Dong, G., Jin, J., Zhang, Y., Zhou, Y., Zhu, Y., Zhang, P., and Dou, Z. (2025d). Search-o1: Agentic search-enhanced large reasoning models. arXiv preprint arXiv:2501.05366.
> - <span id="page-14-1"></span>Li, X., Jin, J., Dong, G., Qian, H., Zhu, Y., Wu, Y., Wen, J.-R., and Dou, Z. (2025e). Webthinker: Empowering large reasoning models with deep research capability. arXiv preprint arXiv:2504.21776.
> - <span id="page-14-9"></span>Li, Z., Guan, X., Zhang, B., Huang, S., Zhou, H., Lai, S., Yan, M., Jiang, Y., Xie, P., Huang, F., et al. (2025f). Webweaver: Structuring web-scale evidence with dynamic outlines for open-ended deep research. arXiv preprint arXiv:2509.13312.
> - <span id="page-14-16"></span>Liu, A., Mei, A., Lin, B., Xue, B., Wang, B., Xu, B., Wu, B., Zhang, B., Lin, C., Dong, C., et al. (2025a). Deepseek-v3. 2: Pushing the frontier of open large language models. arXiv preprint arXiv:2512.02556.
> - <span id="page-14-18"></span>Liu, J., Li, Y., Fu, Y., Wang, J., Liu, Q., and Shen, Y. (2025b). When speed kills stability: Demystifying RL collapse from the training-inference mismatch.
> - <span id="page-14-13"></span>Liu, J., Li, Y., Zhang, C., Li, J., Chen, A., Ji, K., Cheng, W., Wu, Z., Du, C., Xu, Q., et al. (2025c). Webexplorer: Explore and evolve for training long-horizon web agents. arXiv preprint arXiv:2509.06501.
> - <span id="page-14-6"></span>Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, T. (2023). Gaia: a benchmark for general ai assistants. In The Twelfth International Conference on Learning Representations.
> - <span id="page-14-19"></span>MiniMax (2025). Minimax m2 & agent: Ingenious in simplicity. <https://www.minimax.io/news/minimax-m2>.
> - <span id="page-14-11"></span>MiroMind AI Team (2025). Miroflow: A high-performance open-source research agent framework. [https://github.com/](https://github.com/MiroMindAI/MiroFlow) [MiroMindAI/MiroFlow](https://github.com/MiroMindAI/MiroFlow).
> - <span id="page-14-5"></span>MiroMind Team (2025). Mirothinker: Pushing the performance boundaries of open-source research agents via model, context, and interactive scaling. arXiv preprint arXiv:2511.11793.
> - <span id="page-14-21"></span>Moonshot (2025). Kimi-researcher: End-to-end rl training for emerging agentic capabilities. [https://moonshotai.github.io/](https://moonshotai.github.io/Kimi-Researcher/) [Kimi-Researcher/](https://moonshotai.github.io/Kimi-Researcher/).
> - <span id="page-14-7"></span>OpenAI (2025). Introducing deep research. <https://openai.com/index/introducing-deep-research/>.
> - <span id="page-14-17"></span>OpenAI (2025). Introducing gpt-5. <https://openai.com/index/introducing-gpt-5/>.
> - <span id="page-14-20"></span>Perplexity (2025). Introducing perplexity deep research. [https://www.perplexity.ai/hub/blog/](https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research) [introducing-perplexity-deep-research](https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research).
> - <span id="page-14-8"></span>Perplexity Team (2025). Perplexity deep research. [https://www.perplexity.ai/hub/blog/](https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research) [introducing-perplexity-deep-research](https://www.perplexity.ai/hub/blog/introducing-perplexity-deep-research).
> - <span id="page-14-22"></span>Pham, T., Nguyen, N., Zunjare, P., Chen, W., Tseng, Y.-M., and Vu, T. (2025). Sealqa: Raising the bar for reasoning in searchaugmented language models.
> - <span id="page-14-12"></span>Qian, C., Liu, W., Liu, H., Chen, N., Dang, Y., Li, J., Yang, C., Chen, W., Su, Y., Cong, X., et al. (2023). Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924.
> - <span id="page-14-15"></span>Qin, T., Chen, Q., Wang, S., Xing, H., Zhu, K., Zhu, H., Shi, D., Liu, X., Zhang, G., Liu, J., et al. (2025). Flash-searcher: Fast and effective web agents via dag-based parallel execution. arXiv preprint arXiv:2509.25301.
> - <span id="page-14-2"></span>Qiu, J., Qi, X., Zhang, T., Juan, X., Guo, J., Lu, Y., Wang, Y., Yao, Z., Ren, Q., Jiang, X., et al. (2025). Alita: Generalist agent enabling scalable agentic reasoning with minimal predefinition and maximal self-evolution. arXiv preprint arXiv:2505.20286.
> - <span id="page-14-3"></span>Roucher, A., del Moral, A. V., Wolf, T., von Werra, L., and Kaunismäki, E. (2025). 'smolagents': a smol library to build great agentic systems. <https://github.com/huggingface/smolagents>.
> - <span id="page-14-23"></span>Serper, I. (2025). Serper api.
> - <span id="page-14-14"></span>Shi, D., Cao, J., Chen, Q., Sun, W., Li, W., Lu, H., Dong, F., Qin, T., Zhu, K., Yang, M., et al. (2025). Taskcraft: Automated generation of agentic tasks. arXiv preprint arXiv:2506.10055.
> - <span id="page-14-10"></span>SkyworkAI (2025). Deepresearchagent: A hierarchical multi-agent system for deep research. [https://github.com/SkyworkAI/](https://github.com/SkyworkAI/DeepResearchAgent) [DeepResearchAgent](https://github.com/SkyworkAI/DeepResearchAgent).
> - <span id="page-14-4"></span>Sun, S., Song, H., Wang, Y., Ren, R., Jiang, J., Zhang, J., Bai, F., Deng, J., Zhao, W. X., Liu, Z., et al. (2025). Simpledeepsearcher: Deep information seeking via web-powered reasoning trajectory synthesis. arXiv preprint arXiv:2505.16834.
> 
> - <span id="page-15-8"></span>Tang, Q., Xiang, H., Yu, L., Yu, B., Lu, Y., Han, X., Sun, L., Zhang, W., Wang, P., Liu, S., et al. (2025a). Beyond turn limits: Training deep search agents with dynamic context window. arXiv preprint arXiv:2510.08276.
> - <span id="page-15-0"></span>Tang, X., Qin, T., Peng, T., Zhou, Z., Shao, D., Du, T., Wei, X., Xia, P., Wu, F., Zhu, H., Zhang, G., Liu, J., Wang, X., Hong, S., Wu, C., Cheng, H., Wang, C., and Zhou, W. (2025b). Agent kb: Leveraging cross-domain experience for agentic problem solving. In ICML 2025 Workshop on Collaborative and Federated Agentic Workflows.
> - <span id="page-15-13"></span>Tao, Z., Wu, J., Yin, W., Zhang, J., Li, B., Shen, H., Li, K., Zhang, L., Wang, X., Jiang, Y., Xie, P., Huang, F., and Zhou, J. (2025a). Webshaper: Agentically data synthesizing via information-seeking formalization.
> - <span id="page-15-20"></span>Tao, Z., Wu, J., Yin, W., Zhang, J., Li, B., Shen, H., Li, K., Zhang, L., Wang, X., Jiang, Y., Xie, P., Huang, F., and Zhou, J. (2025b). Webshaper: Agentically data synthesizing via information-seeking formalization.
> - <span id="page-15-18"></span>Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., et al. (2025). Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534.
> - <span id="page-15-12"></span>Together AI (2025). Open deep research. <https://www.together.ai/blog/open-deep-research>.
> - <span id="page-15-9"></span>Tongyi DeepResearch Team (2025). Tongyi deepresearch technical report. arXiv preprint arXiv:2510.24701.
> - <span id="page-15-10"></span>Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. (2025). Browsecomp: A simple yet challenging benchmark for browsing agents. arXiv preprint arXiv:2504.12516.
> - <span id="page-15-19"></span>Wu, J., Li, B., Fang, R., Yin, W., Zhang, L., Tao, Z., Zhang, D., Xi, Z., Fu, G., Jiang, Y., Xie, P., Huang, F., and Zhou, J. (2025a). Webdancer: Towards autonomous information seeking agency.
> - <span id="page-15-1"></span>Wu, J., Li, B., Fang, R., Yin, W., Zhang, L., Tao, Z., Zhang, D., Xi, Z., Jiang, Y., Xie, P., et al. (2025b). Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648.
> - <span id="page-15-11"></span>Wu, J., Yin, W., Jiang, Y., Wang, Z., Xi, Z., Fang, R., Zhang, L., He, Y., Zhou, D., Xie, P., et al. (2025c). Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572.
> - <span id="page-15-21"></span>Xbench-Team (2025). Xbench-deepsearch.
> - <span id="page-15-14"></span>Xue, Z., Zheng, L., Liu, Q., Li, Y., Ma, Z., and An, B. (2025). Simpletir: End-to-end reinforcement learning for multi-turn tool-integrated reasoning. <https://simpletir.notion.site/report>. Notion Blog.
> - <span id="page-15-15"></span>Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y. (2023). React: Synergizing reasoning and acting in language models. In International Conference on Learning Representations (ICLR).
> - <span id="page-15-22"></span>Yao, Y., Zhu, H., Wang, P., Ren, J., Yang, X., Chen, Q., Li, X., Shi, D., Li, J., Wang, Q., et al. (2026). O-researcher: An open ended deep research model via multi-agent distillation and agentic rl. arXiv preprint arXiv:2601.03743.
> - <span id="page-15-16"></span>Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. (2025). Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476.
> - <span id="page-15-17"></span>Zeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J., et al. (2025). Glm-4.5: Agentic, reasoning, and coding (arc) foundation models. arXiv preprint arXiv:2508.06471.
> - <span id="page-15-2"></span>Zhang, D., Zhao, Y., Wu, J., Li, B., Yin, W., Zhang, L., Jiang, Y., Li, Y., Tu, K., Xie, P., et al. (2025). Evolvesearch: An iterative self-evolving search agent. arXiv preprint arXiv:2505.22501.
> - <span id="page-15-3"></span>Zheng, Y., Fu, D., Hu, X., Cai, X., Ye, L., Lu, P., and Liu, P. (2025). Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160.
> - <span id="page-15-4"></span>Zhou, W., Jiang, Y. E., Li, L., Wu, J., Wang, T., Qiu, S., Zhang, J., Chen, J., Wu, R., Wang, S., et al. (2023). Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870.
> - <span id="page-15-5"></span>Zhou, W., Ou, Y., Ding, S., Li, L., Wu, J., Wang, T., Chen, J., Wang, S., Xu, X., Zhang, N., et al. (2024). Symbolic learning enables self-evolving agents. arXiv preprint arXiv:2406.18532.
> - <span id="page-15-6"></span>Zhu, H., Qin, T., Zhu, K., Huang, H., Guan, Y., Xia, J., Yao, Y., Li, H., Wang, N., Liu, P., Peng, T., Gui, X., Li, X., Liu, Y., Jiang, Y. E., Wang, J., Zhang, C., Tang, X., Zhang, G., Yang, J., Liu, M., Gao, X., Liu, J., and Zhou, W. (2025a). Oagents: An empirical study of building effective agents.
> - <span id="page-15-7"></span>Zhu, K., Li, H., Wu, S., Xing, T., Ma, D., Tang, X., Liu, M., Yang, J., Liu, J., Jiang, Y. E., Zhang, C., Lin, C., Wang, J., Zhang, G., and Zhou, W. (2025b). Scaling test-time compute for llm agents.
> 
> # <span id="page-16-0"></span>**A Tools Setup**
> 
> Our agent operates with a minimal yet expressive toolset designed to support web-scale information acquisition and consolidation. Specifically, we employ two core tools: a search interface for candidate retrieval and a page-level crawler for content extraction and goal-directed summarization.
> 
> - **web\_search.** This tool provides access to web search functionality through the Serper API, which interfaces with the Google Search engine [\(Serper,](#page-14-23) [2025\)](#page-14-23). Given a model-generated query string, the tool retrieves a ranked list of search results, with the default setting returning the top five entries. Each result consists of a page title, a short snippet, and the corresponding URL. The search results serve as high-level signals for identifying potentially relevant sources and guiding subsequent crawling decisions.
> - **crawl\_page.** This tool is responsible for fine-grained content acquisition and structured summarization. It takes as input both a target URL and an explicit goal describing the information need to be addressed. The URL is crawled using the Jina Reader API [\(Jina,](#page-13-14) [2025\)](#page-13-14), after which the retrieved page content is summarized by the DeepSeek-V3.2 model [\(Liu et al.,](#page-14-16) [2025a\)](#page-14-16). Crucially, the goal specification provides semantic guidance for the summarization process, steering the model to extract and condense information that is directly relevant to the current subtask rather than producing a generic page summary. This goal-conditioned summarization enables more targeted evidence collection and reduces irrelevant context propagation. The prompt template used for page summarization is provided in Appendix [D.2.](#page-26-0)
> 
> # **B Data Construction**
> 
> ## **B.1 Deep Search Data**
> 
> ```
> Entity-Centric Information Extraction Prompt
>    Instruction: Given structured entity metadata and a source content excerpt, extract entity-related information
>    strictly based on explicit textual evidence.
>    Input Fields:
>    Entity Name:
>    {entity_name}
>    Entity Type:
>    {entity_type}
>    Entity Description:
>    {description}
>    Source Content Excerpt:
>    {source_content[:2000]}
>    Task: Extract entity-related information from the source content and return it in the following JSON format.
> 1 {
> 2 " key_attributes ": {
> 3 // Extract attributes relevant to the entity type ({ template })
> 4 // Only include attributes that explicitly appear in the text
> 5 },
> 6 " surface_forms ": [
> 7 // Common surface mentions of the entity in the text
> 8 // ( e . g ., abbreviations , short forms )
> 9 ],
> 10 " aliases ": [
> 11 // Fully equivalent alternative names
> 12 // ( e . g ., full name , pen name , former name )
> 13 ]
> 14 }
> ```
> 
> #### **Extraction Constraints:**
> 
> - 1. **Evidence-Only Extraction**: Only extract information that explicitly appears in the provided source content.
> - 2. **No Inference or Fabrication**: Do not infer, hallucinate, or supplement missing information.
> - 3. **Graceful Degradation**: If a field cannot be extracted, return an empty object ({}) or empty list ([]).
> 
> #### **Output Requirements:**
> 
> - The output must be a valid JSON object.
> - Do not include any content outside the specified JSON fields.
> 
> ## Entity Description Factuality Evaluation Prompt
> 
> **Instruction:** Evaluate the factuality level of candidate entity descriptions and extract key factual information strictly based on the original text.
> 
> #### **Factuality Evaluation Criteria and Scoring:**
> 
> - **High Factuality (80–100 points)**: The description contains concrete, objective facts such as numerical values, time, location, quantities, measurements, dates, or percentages. Example keywords: founded in 2010; 5,000 employees; headquartered in Beijing; market value of USD 10 billion; covers 500 square meters; November 2023.
> - **Medium Factuality (50–79 points)**: The description includes some specific information but is predominantly qualitative. Example keywords: large company; located in China; multiple services; rapid growth; industry participant.
> - **Low Factuality (0–49 points)**: The description is mainly abstract, subjective, or generic, lacking concrete data. Example keywords: important organization; high-quality services; influential; widely followed; has potential.
> 
> #### **Candidate Entity List (Total: {len(candidates)}):**
> 
> {candidate\_info}
> 
> ## **Evaluation Tasks (for each entity):**
> 
> - 1. Assign a factuality score between 0 and 100.
> - 2. Extract key factual information (key\_info) by directly copying the most factual phrases or words from the original description. Multiple fragments should be separated by commas.
> 
> **Critical Constraint:** key\_info must be copied verbatim from the original description. Do **not** paraphrase, rewrite, infer, or add new information.
> 
> **Output Format:** Return the result as a valid JSON object with the following structure:
> 
> ```
> 1 {
> 2 " entity_scores ": [
> 3 {
> 4 " entity_name ": " Entity Name ",
> 5 " score ": 85,
> 6 " level ": " High ",
> 7 " key_info ": " founded in 2010, 5,000 employees , headquartered in Beijing "
> 8 },
> 9 {
> 10 " entity_name ": " Entity Name 2",
> 11 " score ": 45,
> 12 " level ": " Low ",
> 13 " key_info ": " important organization , provides services "
> 14 }
> 15 ]
> 16 }
> ```
> 
> ## **Additional Requirements:**
> 
> - 1. entity\_scores must include all {len(candidates)} candidate entities.
> - 2. Preserve the original order of entities; no sorting is required.
> - 3. level must be one of: "High" (80–100), "Medium" (50–79), or "Low" (0–49).
> - 4. Entity names must exactly match those in the candidate list.
> - 5. key\_info must be copied directly from the original description.
> - 6. Scoring should be objective and unbiased; do not favor any entity.
> 
> ## Hierarchical Fact-Centric Entity Description Generation Prompt
> 
> **Role Definition:** You are an expert responsible for generating a structured description of a target entity based solely on objective, verifiable factual information.
> 
> **Core Principle:** Use only objective, verifiable facts. Subjective, generic, or vague descriptions are strictly prohibited.
> 
> #### **Information to Be Extracted (Priority Order):**
> 
> - 1. **Precise Numbers**: founding years, employee counts, revenue figures, rankings, IDs, award counts, etc. Examples: founded in 1959; more than 5,000 employees; annual revenue of 1 billion USD; won the Nobel Prize twice.
> - 2. **Specific Time**: exact dates, historical periods, event times. Examples: independence on December 12, 1963; participated in the Olympics in 2000; founded in the 1990s.
> - 3. **Specific Locations**: countries, cities, geographic positions, distances, elevations. Examples: located in East Africa; 5 km from the city center; 1,600 meters above sea level.
> - 4. **Specific Events**: historical events, awards, participations, published works. Examples: won the Nobel Prize in Literature; published "XXX"; participated in the 2000 Olympics.
> - 5. **Measurable Attributes**: area, population, scale, institutional size. Examples: area of 580,000 square kilometers; population of 53 million; has 12 colleges.
> 
> #### **Strictly Prohibited Information:**
> 
> - Subjective descriptions (e.g., important, famous, influential)
> - Generic descriptions (e.g., provides services, is an organization)
> - Vague expressions (e.g., many people, a certain region)
> - Functional descriptions without concrete factual support
> 
> #### **Key Requirements:**
> 
> - All facts must be directly extracted from the provided descriptions and relations.
> - Numerical, temporal, and locational information must appear verbatim in the output.
> - If a piece of information contains no objective facts, it must be ignored.
> 
> **Special Emphasis: Hierarchical Abstraction Strategy** Present all extracted information hierarchically. Do not omit any factual information, and do not invent content.
> 
> ## **Input Content:**
> 
> - 1. **Current Entity Information:**
>   - Entity type: {entity\_type}
>   - Number of related items: {len(children\_descriptions)}
> - 2. **Related Information List** ({len(children\_descriptions)} items):
> 
> {children\_info\_text}
> 
> 3. **Relationships Among Related Information:**
> 
> {peers\_info\_text}
> 
> # **Output Content:** Generate the following three items:
> 
> - 1. **facts**: a list of independent, objective, and verifiable facts extracted from the input.
> - 2. **summary**: a one-sentence abstraction of relationship types (e.g., work, geography, education).
> - 3. **question**: a multi-hop search question constructed strictly from the extracted objective facts.
> 
> #### **Entity Name Usage Rules:**
> 
> - The name of the current entity must never be mentioned.
> - Names of related entities must also never be mentioned in the question.
> - Use abstract references such as "an organization", "a country", "a company", or relational descriptions.
> 
> ## **Fact Extraction Procedure:**
> 
> - 1. Scan related information for objective facts containing numbers, time, locations, or events.
> - 2. Categorize facts into numerical, temporal, locational, event-based, and measurement-based.
> - 3. Construct fact descriptions that integrate all objective details without using specific names.
> - 4. Ensure each fact contains at least one verifiable objective element.
> 
> ## **Question Construction Principles:**
> 
> - The question must be built solely from extracted objective facts.
> - All numerical, temporal, locational, event, and measurement information must be included.
> - Completeness is more important than brevity.
> - If insufficient objective facts exist, return an empty result.
> 
> #### **Output Format:**
> 
> ```
> 1 {
> 2 " facts ": [
> 3 {
> 4 " fact ": "[ single fact description without specific names ]",
> 5 " source ": " child :0"
> 6 },
> 7 {
> 8 " fact ": "[ single fact description without specific names ]",
> 9 " source ": " child_peer :0_1"
> 10 }
> 11 ],
> 12 " summary ": " Abstract description of relationship types ",
> 13 " question ": " Which { entity_type } satisfies the following conditions : [...]?"
> 14 }
> ```
> 
> #### Search-Enforced Entity Description Obfuscation Prompt
> 
> **Role Definition:** You are an expert responsible for obfuscating an entity description such that the correct answer cannot be inferred directly and must be determined through external search.
> 
> ## **Current Situation:**
> 
> - **Entity type:** {entity\_type}
> - **Entity name (ground truth):** {entity\_name} (This name must never appear in the obfuscated description.)
> - **Original description:**
> 
> {description}
> 
> #### **Conditions in the Description:**
> 
> {facts\_detail\_text}
> 
> ## **Direct Reasoning Basis (to be neutralized):**
> 
> {reasoning}
> 
> #### **Entity Details:**
> 
> • **Full description:**
> 
> {entity\_description}
> 
> • **Key attributes:**
> 
> ```
> {key_attrs_text}
> ```
> 
> #### **Strictly Prohibited Content:**
> 
> - 1. Do not use prompt-instruction language (e.g., "related information 0").
> - 2. Do not mention the entity name {entity\_name} or any other specific entity name.
> - 3. Do not use expressions such as "this {entity\_type}" or "the above {entity\_type}".
> 
> #### **Required Natural Language Expressions:**
> 
> - Use generic references (e.g., "an organization", "a company", "a country", "a writer").
> - Use characteristic descriptions (e.g., "a media company founded in year XX").
> - Use relational descriptions (e.g., "the employing company", "the country of birth").
> 
> **Task:** Select one single most critical condition to obfuscate such that the model can no longer directly infer the answer and must rely on search for verification.
> 
> #### **Obfuscation Strategies:**
> 
> - 1. **Time Obfuscation**: Convert an exact time into a vague period or a search-dependent temporal description. Examples: "founded in 1949" → "founded in the late 1940s".
> - 2. **Entity Obfuscation**: Replace an entity name with search-required attributes. Examples: "founded by Steve Jobs" → "founded by a Silicon Valley entrepreneur whose name starts with 'S"'.
> - 3. **Quantitative Obfuscation**: Replace precise numbers with ranges or qualitative quantities. Examples: "won the Nobel Prize twice" → "won the Nobel Prize more than once".
> - 4. **Information Deletion**: Remove redundant or overly revealing conditions that make the answer trivial.
> - 5. **Concept Obfuscation**: Abstract a specific concept into a higher-level description supported by searchable facts.
> - 6. **Combined Obfuscation**: Apply multiple lightweight obfuscations while preserving coherence.
> 
> ## **Selection Criteria:**
> 
> - Prioritize obfuscating the key condition used in direct reasoning.
> - Choose the condition that most reduces certainty.
> - Keep the description understandable and logically coherent.
> - Ensure the answer remains discoverable via search.
> - Avoid subjective or vague phrasing.
> 
> ## **Output Format:**
> 
> ```
> 1 {
> 2 " target_fact_index ": 0,
> 3 " obfuscation_strategy ": " Time obfuscation ",
> 4 " obfuscated_fact ": " the obfuscated condition text ",
> 5 " obfuscated_description ": " the full obfuscated description ",
> 6 " reasoning ": " Explanation of why this condition was selected and how it was
>         obfuscated "
> 7 }
> ```
> 
> **Output Constraint:** Output only the JSON object. Do not include any additional text.
> 
> #### Entity Answer Verification Prompt
> 
> **Instruction:** Answer the question strictly based on the provided description and identify the corresponding entity.
> 
> ## **Input Fields:**
> 
> #### **Description:**
> 
> {description}
> 
> #### **Question:**
> 
> What is this {entity\_type}?
> 
> #### **Task:**
> 
> - Infer the most specific entity name that satisfies the given description.
> - Base your answer only on the provided description.
> 
> **Output Format:** Return the result as a valid JSON object with the following fields:
> 
> ```
> 1 {
> 2 " answer ": " Entity name ",
> 3 " reasoning ": " Brief explanation of which conditions were used to infer the
>         answer "
> 4 }
> ```
> 
> #### **Constraints:**
> 
> - You must return a specific and explicit entity name.
> - Do not respond with a concept, category, or abstract description.
> - If the answer cannot be determined from the description, set "answer" to "Cannot determine".
> - Output only the JSON object. Do not include any additional text.
> 
> ## Entity Answer Equivalence Verification Prompt
> 
> **Instruction:** Determine whether a predicted answer refers to the same entity as the ground-truth answer.
> 
> #### **Input Fields:**
> 
> #### **Ground-Truth Answer:**
> 
> {entity\_name}
> 
> ## **Predicted Answer:**
> 
> {predicted\_answer}
> 
> **Task:** Evaluate whether the predicted answer correctly identifies the same entity as the ground-truth answer.
> 
> **Output Format:** Return the result as a valid JSON object with the following fields:
> 
> ```
> 1 {
> 2 " is_correct ": true ,
> 3 " explanation ": " Brief explanation of why the answers match or do not match "
> 4 }
> ```
> 
> #### <span id="page-21-0"></span>**Judgment Criteria:**
> 
> - If the two answers refer to the same entity (even if phrased differently), return true.
> - If the predicted answer is "Cannot determine" or is clearly incorrect, return false.
> - The predicted answer must be a specific and explicit entity name; concepts, categories, or abstract descriptions are not acceptable.
> - Consider valid aliases, abbreviations, and alternative names.
> 
> **Output Constraint:** Output only the JSON object. Do not include any additional text.
> 
> ## **B.2 Deep Research Data**
> 
> #### Graph-Grounded Open-Ended Research Question Construction Prompt
> 
> **Instruction:** Construct a high-quality open-ended research question strictly grounded in the provided subgraph structure and factual evidence.
> 
> ## **Input Fields:**
> 
> #### **Target Entity:**
> 
> {target\_entity}
> 
> #### **Entity Type:**
> 
> ```
> {entity_type}
> ```
> 
> #### **Supporting Subgraph:**
> 
> {subgraph\_description}
> 
> **Task:** Generate **one open-ended research question** that requires synthesizing information from multiple nodes and relations in the supporting subgraph.
> 
> The question should:
> 
> - Require integrating evidence across the subgraph rather than relying on a single fact.
> - Admit multiple valid answers supported by different reasoning paths.
> - Be suitable for a **report-style response** involving explanation, comparison, or synthesis.
> 
> #### **Construction Constraints:**
> 
> - 1. **Graph-Grounded**: The question must be answerable only using information contained in the provided subgraph.
> - 2. **Non-Deterministic**: Do not construct questions with a single exact or short factual answer.
> - 3. **Multi-Evidence Requirement**: The question must require reasoning over multiple entities, attributes, or relations.
> - <span id="page-22-0"></span>4. **No Trivial Reformulation**: Avoid paraphrasing factual descriptions or directly listing conditions.
> 
> #### **Output Format:**
> 
> ```
> {
>   "research_question": "A single open-ended research question"
> }
> ```
> 
> #### **Output Requirements:**
> 
> - Output must be a valid JSON object.
> - Do not include explanations, rationales, or additional text.
> 
> # C Case Study
> 
> <span id="page-23-0"></span>![[chen-22675-fig-008.jpeg]]
> 
> **Figure 5** Case study illustration comparing SMTL-30B and MiroThinker-v1.0-30B. SMTL performs parallel subtask execution with staged re-planning, enabling faster localization and verification of key evidence, while MiroThinker-v1.0-30B follows a strictly sequential search process.
> 
> # **D** Prompts
> 
> #### **D.1 System Prompts**
> 
> We employ two system prompts to support <u>Deep Search</u> and <u>Deep Research</u> tasks, respectively. While the two prompts differ in their output structure and interaction protocols, they operate under a <u>shared parallel agentic search framework</u> as described in the main paper.
> 
> Specifically, both system prompts follow a unified design philosophy: tasks are represented over graph-structured evidence, decomposed into multiple goals or subtasks, and solved through parallel execution and coordinated tool use. In both settings, the agent performs explicit planning, iterative plan refinement based on tool observations, and structured progress tracking, enabling efficient long-horizon search under constrained interaction budgets.
> 
> The primary distinction lies in the <u>response format and organization</u>. The Deep Search prompt adopts a compact plan–plan-refine–answer structure optimized for deterministic question answering and efficient verification, whereas the Deep Research prompt enforces a finer-grained subtask-oriented protocol and report-style synthesis, tailored to open-ended research problems with multi-dimensional evaluation criteria. Despite these differences in output formatting
> 
> and control flow granularity, both prompts instantiate the same underlying parallel agentic execution paradigm.
> 
> ## System Prompt for Deep Search Tasks
> 
> **System Role:** You are an expert assistant that solves complex search tasks through structured tool usage and explicit goal management. You must follow a step-by-step execution process that combines planning, parallel execution, verification, and concise final answering.
> 
> ## **Core Objective:**
> 
> - Decompose the task into clear, verifiable goals.
> - Advance multiple goals in parallel whenever possible.
> - Execute each goal through sequential fallback paths.
> - Use tools purposefully and efficiently to gather and verify evidence.
> - Produce a final answer only after all goals are explicitly resolved.
> 
> #### **Execution Requirements:**
> 
> - 1. Follow a logical and structured order of tool usage.
> - 2. Parallelize independent goals; execute paths within a goal sequentially.
> - 3. Each action step must include:
>   - A brief reasoning explaining why the selected tool or path is chosen.
>   - One or more <tool\_call> executions with valid parameters.
>   - Interpretation of observations returned by tools.
>   - Refinement of subsequent actions based on observations.
> - 4. Avoid redundant tool calls and repeated queries.
> - 5. Never assume task completion without explicit verification.
> 
> #### **Functional Interfaces:**
> 
> #### **1. <plan> Function**
> 
> - **Role**: Decompose the task into parallelizable goals and execution paths.
> - **Constraints**:
>   - 1–5 goals.
>   - Each goal has 1–5 sequential fallback paths.
>   - Each path must specify a clear success criterion.
> - **Timing**: Used only at the first step.
> 
> #### **2. <plan\_refine> Function**
> 
> - **Role**: Summarize progress and decide next actions.
> - **Content**:
>   - Plan recap.
>   - Execution status of each goal (Completed / In Progress / Blocked).
>   - Analysis of attempted paths.
>   - Next sub-paths to execute in parallel.
> - **Timing**: Invoked periodically after multiple actions.
> 
> #### **3. <tool\_call> Function**
> 
> - **Available Tools**:
>   - web\_search(query): Broad information discovery.
>   - crawl\_page(url, query): Deep verification guided by an information goal.
> 
> #### • **Usage Rules**:
> 
> - Use 1–5 tool calls per step, targeting distinct subtasks.
> - Prioritize authoritative and official sources when precision is required.
> - Always verify promising URLs via crawl\_page.
> 
> # **4. <answer> Function**
> 
> - **Role**: Output the final confirmed answer.
> - **Constraints**:
>   - Only after all goals are resolved.
>   - Must consolidate evidence across all goals.
> 
> - Language must match the task language.
> - Keep the answer concise and factual.
> 
> #### **Critical Execution Rules:**
> 
> - Advance all goals in parallel whenever possible.
> - Never terminate early without verification.
> - Do not skip promising sources during verification.
> - Use no more than 10 tool calls per step.
> 
> **Final Note:** Do not produce an answer unless you are absolutely certain. Final answers should be concise and avoid unnecessary explanation.
> 
> ## System Prompt for Deep Research Tasks
> 
> **System Role:** You are an expert assistant who solves tasks through structured tool calls, following a step-by-step process. Each step (action) involves analyzing needs, selecting tools, and executing calls to achieve the task. You are required to solve the task by formulating your thinking and reasoning process as described below.
> 
> **Allowed Functions (Tag-based Protocol).** You can only use the following functions to answer a given question: subtask\_list, subtask, analysis, plan, tool\_call, tool\_response, subtask\_answer, answer.
> 
> - 1. **subtask\_list**: At the very beginning, break down the complex question into a list of clear, independent subtasks. Start with <subtask\_list> and end with </subtask\_list>.
> - 2. **subtask**: Marks the beginning of executing one specific subtask from subtask\_list. Start with <subtask> and end with </subtask>.
> - 3. **analysis**: Within a subtask, before using plan or tool\_call, provide reasoning, arguments, and next steps. Start with <analysis> and end with </analysis>.
> - 4. **plan**: For the current subtask, break it down into fine-grained steps to be executed using tools. Start with <plan> and end with </plan>.
> - 5. **tool\_call**: Execute tools from the tool list below to gather information relevant to answering the question.
> - 6. **tool\_response**: The response returned after a tool is executed.
> - 7. **subtask\_answer**: After completing a subtask, provide an intermediate, definitive answer. Start with <subtask\_answer> and end with </subtask\_answer>.
> - 8. **answer**: After all subtasks are completed, synthesize all subtask\_answers into a final comprehensive answer.
> 
> ## **Available Tools.**
> 
> ```
> 1. web_search: one parameter query. Example:
> ```
> 
> ```
> {'name': 'web_search', 'arguments': {'query': 'xxx'}}
> ```
> 
> 2. **crawl\_page**: two parameters url and query. Example:
> 
> ```
> {'name': 'crawl_page', 'arguments': {'url': 'xxx', 'query': 'xxx'}}
> ```
> 
> #### **Tool Usage Guide.**
> 
> - 1. **web\_search**: If retrieved information is irrelevant, re-search with new queries until sufficient relevant information is obtained and you are highly confident in the final answer.
> - 2. **crawl\_page**: Use crawl\_page to obtain detailed information from URLs, and crawl additional URLs when needed.
> - 3. **Deeper search**: Use web\_search to discover URLs, then crawl\_page to verify and extract details. Repeat web\_search/crawl\_page multiple times if deeper hints emerge.
> 
> ## **Trail Notes.**
> 
> - 1. **Overall workflow**: Start with <subtask\_list>. Then for each subtask, follow an analysis → plan → tool → tool\_response loop until the subtask is answerable, and output <subtask\_answer>. After all subtasks, produce <answer>.
> - 2. **Information gathering**: Based on the plan, you may use tools multiple times to collect sufficient external knowledge for the current subtask.
> 
> 3. **Tag restrictions**: <subtask\_list>, <subtask>, <analysis>, <plan>, <web\_search>, <crawl\_page>, <tool\_response>, <subtask\_answer>, <answer> are special tags and must not appear in free text, especially within the <analysis> function.
> 
> #### **Function Association Instructions.**
> 
> - 1. The process must start with <subtask\_list>.
> - 2. After <subtask\_list>, the first <subtask> must begin.
> - 3. Inside each <subtask>, before using plan or tools, you must first use <analysis>.
> - 4. After information gathering is complete for a subtask, a <subtask\_answer> must be generated.
> - 5. Following a <subtask\_answer>, either start the next <subtask> or, if all are complete, generate <answer>.
> 
> **Answering Tips (Report Formatting).** The final answer must be a detailed, well-structured report with traceable information and must strictly follow:
> 
> - 1. **Structure**: include an introduction, body paragraphs (organized by subtasks if appropriate), a conclusion, and a references section.
> - 2. **In-text citations**: every key information point, data point, or quote must be supported by a source. Use numbered square brackets with spaces, e.g., [1] , [2] , placed immediately after the supported claim.
> 
> **References Section.** Include a section titled References at the end of the report. It must be a numbered list corresponding one-to-one with citation numbers. Each entry must follow:
> 
> [Number]. URL - Webpage Title.
> 
> # <span id="page-26-0"></span>**D.2 Summary Prompt**
> 
> ## Goal-Conditioned Webpage Summarization Prompt
> 
> **Instruction:** Please process the provided webpage content and extract information that is directly relevant to the specified information goal.
> 
> ## **Input Fields:**
> 
> #### **Webpage URL:**
> 
> {url}
> 
> #### **Webpage Content:**
> 
> {content}
> 
> ## **Information Goal:**
> 
> {query}
> 
> #### **Task Guidelines:**
> 
> - 1. **Goal-Oriented Content Scanning**: Identify the specific sections, passages, or data within the webpage that are directly relevant to the given information goal. Ignore unrelated or tangential content.
> - 2. **Evidence Extraction**: Extract the most relevant information that supports the goal. Do not omit important details. Preserve the original wording and context as much as possible, and include extended excerpts when necessary (potentially spanning multiple paragraphs).
> - 3. **Structured Summarization**: Organize the extracted information into a coherent and concise summary with a clear logical structure, prioritizing relevance, factual accuracy, and completeness with respect to the goal.
> 
> **Output Format:** The output must be a valid JSON object with the following fields:
> 
> ```
> 1 {
> 2 " rationale ": " Explanation of why the extracted content is relevant to the
>         information goal ",
> 3 " evidence ": " Verbatim excerpts or minimally edited passages from the webpage
>         that directly support the goal ",
> 4 " summary ": " A concise , goal - focused summary synthesized from the extracted
> ```
> 
> evidence " 5 }
> 
> ## **Additional Requirements:**
> 
> - The output must be fully JSON-parsable.
> - All special characters must be properly escaped.
> - Do not include any content outside the specified JSON fields.
> 
> # **D.3 LLM-as-a-Judge Prompts**
> 
> We design two distinct LLM-as-a-judge prompts for Deep Search and Deep Research evaluations, respectively. This separation is necessary because the two task settings produce fundamentally different types of outputs and therefore require different evaluation protocols to ensure fairness and reliability.
> 
> For Deep Search tasks, model outputs are typically short, deterministic answers with well-defined ground truth. Accordingly, the judge prompt focuses on semantic equivalence between the predicted answer and the labeled answer, allowing for minor surface-form variations while enforcing strict correctness. In contrast, Deep Research tasks produce long-form, report-style responses that involve synthesis, analysis, and organization across multiple sources. Evaluating such outputs requires a multi-dimensional rubric that assesses content coverage, analytical depth, instruction adherence, and readability rather than exact answer matching.
> 
> By tailoring the judge prompts to the structural and semantic characteristics of each task type, we ensure that evaluation remains consistent, unbiased, and aligned with the intended objectives of the benchmark. Both judge prompts are applied within the same evaluation framework but are specialized to match the output format and reasoning demands of their respective scenarios.
> 
> #### LLM-as-a-Judge Prompt for Deep Search Tasks
> 
> **Instruction:** Please determine whether the Predicted Answer is semantically equivalent to the Labeled Answer, given the Question.
> 
> ## **Input Format:**
> 
> Question: {question} Labeled Answer: {gt\_answer} Predicted Answer: {pred\_answer}
> 
> #### **Evaluation Process:**
> 
> - 1. **Instruction Focus**: Your judgment must be based solely on whether the meaning conveyed by the Predicted Answer aligns with the meaning of the Reference Answer.
> - 2. **Allowable Variations**:
>   - For **text answers**: Differences in capitalization, punctuation, grammar (including prepositions, articles, and grammatical structures), word order, phrasing style, measurement units, or the inclusion/exclusion of non-essential descriptive phrases are **acceptable** if they do not alter the core meaning.
>   - For **names and titles**: Variations in name formats (e.g., inclusion of middle names, parentheses with additional information, honorifics) are acceptable if they refer to the same entity/person.
>   - For **numerical answers**: Minor acceptable margins of error are permissible when appropriate for the context.
>   - **Synonyms and near-synonyms** that convey the same meaning in context are acceptable.
> - 3. **Criteria for CORRECT Judgement**: Judge as **"correct"** only if:
>   - a. The Predicted Answer **directly addresses** the Question, and
>   - b. Its **core meaning** is **semantically equivalent** to the Reference Answer, and
>   - c. It does **not contradict** any explicit requirements or constraints in the Question or Reference Answer. **Important**: If the Predicted Answer contains the Reference Answer within it (e.g., as an alternative name or with additional descriptive context) or expresses the same concept with different wording, it
> 
> should be considered correct.
> 
> - 4. **Criteria for INCORRECT Judgement**: Judge as **"incorrect"** if any of the following apply:
>   - a. The Predicted Answer **misses essential information** that changes the meaning of the Reference Answer (not just additional descriptive details), or
>   - b. The Predicted Answer **adds contradictory or incompatible information** that changes the intended meaning (mere additional non-contradictory details are acceptable), or
>   - c. The Predicted Answer is **ambiguous, indirect, or evasive** and fails to provide a clear, direct response to the Question, or
>   - d. The Predicted Answer is **semantically different** from the Reference Answer in a meaningful way that would lead to different understanding or action.
> 
> #### **Special Considerations:**
> 
> - **Preposition variations**: Differences in prepositions (e.g., "at" vs "in", "on" vs "upon") are generally acceptable unless they fundamentally change the meaning in the specific context.
> - **Parenthetical information**: Inclusion of additional information in parentheses (e.g., alternative names, explanations) is acceptable if the core entity/answer remains the same.
> - **Context sensitivity**: Consider the specific context of the question. For factual questions, focus on whether the same fact is conveyed.
> 
> ### **Output Format:**
> 
> ```
> 1 {
> 2 " rationale ": " your rationale for the judgement ",
> 3 " judgement ": " correct or incorrect "
> 4 }
> ```
> 
> #### LLM-as-a-Judge Prompt for Deep Research Tasks
> 
> **System Role:** You are a strict, meticulous, and objective research article evaluation expert. You excel at using specific assessment criteria to deeply compare two articles on the same task, providing precise scores and clear justifications.
> 
> **Task Background:** There is a deep research task, and you need to evaluate two research articles written for this task. The evaluation is conducted across four dimensions: **Comprehensiveness**, **Insight**, **Instruction Following**, and **Readability**.
> 
> ## **Research Task:**
> 
> {task\_prompt}
> 
> #### **Articles to Evaluate:**
> 
> #### **Article 1:**
> 
> {article\_1}
> 
> ## **Article 2:**
> 
> {article\_2}
> 
> **Evaluation Criteria:** The articles should be evaluated and compared based on the following evaluation criteria list. Each criterion includes a detailed explanation and should be carefully considered. {criteria\_list}
> 
> # **Instruction:**
> 
> **Your Task** Please strictly evaluate and compare <article\_1> and <article\_2> based on **each criterion** in the <criteria\_list>. You must:
> 
> - 1. **Analyze Each Criterion**: Assess how each article fulfills the requirements of the given criterion.
> - 2. **Comparative Evaluation**: Compare the performance of the two articles for each criterion, explicitly referencing the article content and the criterion explanation.
> - 3. **Score Separately**: Assign a score to each article for each criterion on a scale from 0 to 10.
> 
> #### **Scoring Rules:**
> 
> • **0–2**: Very poor performance; almost completely fails to meet the criterion.
> 
> - **2–4**: Poor performance; minimally meets the criterion with major deficiencies.
> - **4–6**: Average performance; basically meets the criterion.
> - **6–8**: Good performance; largely meets the criterion with notable strengths.
> - **8–10**: Excellent performance; fully meets or exceeds the criterion.
> 
> **Output Format Requirements:** The output must **strictly follow** the JSON format below. Do **not** include any introduction, summary, or unrelated content. Evaluation should start from "Standard 1" and proceed sequentially through all criteria.
> 
> ```
> 1 {
> 2 " comprehensiveness ": [
> 3 {
> 4 " criterion ": " Text content of the first comprehensiveness evaluation
>                criterion ",
> 5 " analysis ": " Comparative analysis ",
> 6 " article_ 1 _score ": 0-10,
> 7 " article_ 2 _score ": 0-10
> 8 },
> 9 ...
> 10 ],
> 11 " insight ": [
> 12 {
> 13 " criterion ": " Text content of the first insight evaluation criterion "
>                ,
> 14 " analysis ": " Comparative analysis ",
> 15 " article_ 1 _score ": 0-10,
> 16 " article_ 2 _score ": 0-10
> 17 },
> 18 ...
> 19 ],
> 20 ...
> 21 }
> ```
> 
> **Additional Requirement:** Ensure that the output JSON is fully parsable and that all characters that may cause JSON parsing errors are properly escaped.
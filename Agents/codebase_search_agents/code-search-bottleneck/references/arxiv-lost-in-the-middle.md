---
created: 2026-02-25
type: reference
description: "LLM performance degrades by over 30% when relevant information is in the middle of long contexts, following a U-shaped curve that favors start and end positions."
source: "https://arxiv.org/abs/2307.03172"
---
Title: Lost in the Middle: How Language Models Use Long Contexts

URL Source: https://arxiv.org/abs/2307.03172

Published Time: Wed, 22 Nov 2023 01:07:48 GMT

Markdown Content:
[2307.03172] Lost in the Middle: How Language Models Use Long Contexts
===============

[Skip to main content](https://arxiv.org/abs/2307.03172#content)

[![Image 1: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)

We gratefully acknowledge support from the Simons Foundation, [member institutions](https://info.arxiv.org/about/ourmembers.html), and all contributors.[Donate](https://info.arxiv.org/about/donate.html)

[](https://arxiv.org/IgnoreMe)

[![Image 2: arxiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg)](https://arxiv.org/)>[cs](https://arxiv.org/list/cs/recent)> arXiv:2307.03172 

[Help](https://info.arxiv.org/help) | [Advanced Search](https://arxiv.org/search/advanced)

Search

[![Image 3: arXiv logo](https://arxiv.org/static/browse/0.3.4/images/arxiv-logomark-small-white.svg)](https://arxiv.org/)

[![Image 4: Cornell University Logo](https://arxiv.org/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg)](https://www.cornell.edu/)

GO

quick links
-----------

*   [Login](https://arxiv.org/login)
*   [Help Pages](https://info.arxiv.org/help)
*   [About](https://info.arxiv.org/about)

Computer Science > Computation and Language
===========================================

**arXiv:2307.03172** (cs) 

 [Submitted on 6 Jul 2023 ([v1](https://arxiv.org/abs/2307.03172v1)), last revised 20 Nov 2023 (this version, v3)]

Title:Lost in the Middle: How Language Models Use Long Contexts
===============================================================

Authors:[Nelson F. Liu](https://arxiv.org/search/cs?searchtype=author&query=Liu,+N+F), [Kevin Lin](https://arxiv.org/search/cs?searchtype=author&query=Lin,+K), [John Hewitt](https://arxiv.org/search/cs?searchtype=author&query=Hewitt,+J), [Ashwin Paranjape](https://arxiv.org/search/cs?searchtype=author&query=Paranjape,+A), [Michele Bevilacqua](https://arxiv.org/search/cs?searchtype=author&query=Bevilacqua,+M), [Fabio Petroni](https://arxiv.org/search/cs?searchtype=author&query=Petroni,+F), [Percy Liang](https://arxiv.org/search/cs?searchtype=author&query=Liang,+P)

View a PDF of the paper titled Lost in the Middle: How Language Models Use Long Contexts, by Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang

[View PDF](https://arxiv.org/pdf/2307.03172)
> Abstract:While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.

Comments:18 pages, 16 figures. Accepted for publication in Transactions of the Association for Computational Linguistics (TACL), 2023
Subjects:Computation and Language (cs.CL)
Cite as:[arXiv:2307.03172](https://arxiv.org/abs/2307.03172) [cs.CL]
(or [arXiv:2307.03172v3](https://arxiv.org/abs/2307.03172v3) [cs.CL] for this version)
[https://doi.org/10.48550/arXiv.2307.03172](https://doi.org/10.48550/arXiv.2307.03172)

Focus to learn more

 arXiv-issued DOI via DataCite

Submission history
------------------

 From: Nelson F. Liu [[view email](https://arxiv.org/show-email/aa9cdeea/2307.03172)] 

**[[v1]](https://arxiv.org/abs/2307.03172v1)** Thu, 6 Jul 2023 17:54:11 UTC (410 KB)

**[[v2]](https://arxiv.org/abs/2307.03172v2)** Mon, 31 Jul 2023 17:48:48 UTC (416 KB)

**[v3]** Mon, 20 Nov 2023 23:09:34 UTC (334 KB)

[](https://arxiv.org/abs/2307.03172)Full-text links:
Access Paper:
-------------

 View a PDF of the paper titled Lost in the Middle: How Language Models Use Long Contexts, by Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang

*   [View PDF](https://arxiv.org/pdf/2307.03172)
*   [TeX Source](https://arxiv.org/src/2307.03172)

[view license](http://arxiv.org/licenses/nonexclusive-distrib/1.0/ "Rights to this article")

 Current browse context: 

cs.CL

[<prev](https://arxiv.org/prevnext?id=2307.03172&function=prev&context=cs.CL "previous in cs.CL (accesskey p)") | [next>](https://arxiv.org/prevnext?id=2307.03172&function=next&context=cs.CL "next in cs.CL (accesskey n)")

[new](https://arxiv.org/list/cs.CL/new) | [recent](https://arxiv.org/list/cs.CL/recent) | [2023-07](https://arxiv.org/list/cs.CL/2023-07)

 Change to browse by: 

[cs](https://arxiv.org/abs/2307.03172?context=cs)

### References & Citations

*   [NASA ADS](https://ui.adsabs.harvard.edu/abs/arXiv:2307.03172)
*   [Google Scholar](https://scholar.google.com/scholar_lookup?arxiv_id=2307.03172)
*   [Semantic Scholar](https://api.semanticscholar.org/arXiv:2307.03172)

### [4 blog links](https://arxiv.org/tb/2307.03172)

 ([what is this?](https://info.arxiv.org/help/trackback.html)) 

export BibTeX citation Loading...

BibTeX formatted citation
-------------------------

Ã—

Data provided by: [](https://arxiv.org/abs/2307.03172)

### Bookmark

[![Image 5: BibSonomy logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/bibsonomy.png)](http://www.bibsonomy.org/BibtexHandler?requTask=upload&url=https://arxiv.org/abs/2307.03172&description=Lost%20in%20the%20Middle:%20How%20Language%20Models%20Use%20Long%20Contexts "Bookmark on BibSonomy")[![Image 6: Reddit logo](https://arxiv.org/static/browse/0.3.4/images/icons/social/reddit.png)](https://reddit.com/submit?url=https://arxiv.org/abs/2307.03172&title=Lost%20in%20the%20Middle:%20How%20Language%20Models%20Use%20Long%20Contexts "Bookmark on Reddit")

Bibliographic Tools 

Bibliographic and Citation Tools
================================

- [x] Bibliographic Explorer Toggle 

Bibliographic Explorer _([What is the Explorer?](https://info.arxiv.org/labs/showcase.html#arxiv-bibliographic-explorer))_

- [x] Connected Papers Toggle 

Connected Papers _([What is Connected Papers?](https://www.connectedpapers.com/about))_

- [x] Litmaps Toggle 

Litmaps _([What is Litmaps?](https://www.litmaps.co/))_

- [x] scite.ai Toggle 

scite Smart Citations _([What are Smart Citations?](https://www.scite.ai/))_

Code, Data, Media 

Code, Data and Media Associated with this Article
=================================================

- [x] alphaXiv Toggle 

alphaXiv _([What is alphaXiv?](https://alphaxiv.org/))_

- [x] Links to Code Toggle 

CatalyzeX Code Finder for Papers _([What is CatalyzeX?](https://www.catalyzex.com/))_

- [x] DagsHub Toggle 

DagsHub _([What is DagsHub?](https://dagshub.com/))_

- [x] GotitPub Toggle 

Gotit.pub _([What is GotitPub?](http://gotit.pub/faq))_

- [x] Huggingface Toggle 

Hugging Face _([What is Huggingface?](https://huggingface.co/huggingface))_

- [x] Links to Code Toggle 

Papers with Code _([What is Papers with Code?](https://paperswithcode.com/))_

- [x] ScienceCast Toggle 

ScienceCast _([What is ScienceCast?](https://sciencecast.org/welcome))_

Demos 

Demos
=====

- [x] Replicate Toggle 

Replicate _([What is Replicate?](https://replicate.com/docs/arxiv/about))_

- [x] Spaces Toggle 

Hugging Face Spaces _([What is Spaces?](https://huggingface.co/docs/hub/spaces))_

- [x] Spaces Toggle 

TXYZ.AI _([What is TXYZ.AI?](https://txyz.ai/))_

Related Papers 

Recommenders and Search Tools
=============================

- [x] Link to Influence Flower 

Influence Flower _([What are Influence Flowers?](https://influencemap.cmlab.dev/))_

- [x] Core recommender toggle 

CORE Recommender _([What is CORE?](https://core.ac.uk/services/recommender))_

*   [Author](https://arxiv.org/abs/2307.03172)
*   [Venue](https://arxiv.org/abs/2307.03172)
*   [Institution](https://arxiv.org/abs/2307.03172)
*   [Topic](https://arxiv.org/abs/2307.03172)

 About arXivLabs  

arXivLabs: experimental projects with community collaborators
=============================================================

arXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.

Both individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.

Have an idea for a project that will add value for arXiv's community? [**Learn more about arXivLabs**](https://info.arxiv.org/labs/index.html).

[Which authors of this paper are endorsers?](https://arxiv.org/auth/show-endorsers/2307.03172) | [Disable MathJax](javascript:setMathjaxCookie()) ([What is MathJax?](https://info.arxiv.org/help/mathjax.html)) 

*   [About](https://info.arxiv.org/about)
*   [Help](https://info.arxiv.org/help)

*   [Contact](https://info.arxiv.org/help/contact.html)
*   [Subscribe](https://info.arxiv.org/help/subscribe)

*   [Copyright](https://info.arxiv.org/help/license/index.html)
*   [Privacy Policy](https://info.arxiv.org/help/policies/privacy_policy.html)

*   [Web Accessibility Assistance](https://info.arxiv.org/help/web_accessibility.html)
*   [arXiv Operational Status](https://status.arxiv.org/)

## Figures

### Figure 1: U-Shaped Performance Curve (Key Result)
![Figure 1](../../../../_media/lost-in-middle-fig1.png)
*Changing the location of relevant information within the language model's input context results in a U-shaped performance curve.*

### Figure 2: Multi-Document QA Task Example
![Figure 2](../../../../_media/lost-in-middle-fig2.png)
*Example of the multi-document question answering task, with an input context and the desired model answer.*

### Figure 3: Modulating Input Context Length
![Figure 3](../../../../_media/lost-in-middle-fig3.png)
*Adding additional documents that do not contain the answer increases the length of the input context.*

### Figure 4: Modulating Position of Relevant Information
![Figure 4](../../../../_media/lost-in-middle-fig4.png)
*Re-ordering the documents in the input context changes the position of relevant information.*

### Figure 5: Position Effect on Multi-Doc QA
![Figure 5](../../../../_media/lost-in-middle-fig5.png)
*The effect of changing the position of relevant information on multi-document question answering performance.*

### Figure 6: Performance vs Context Length
![Figure 6](../../../../_media/lost-in-middle-fig6.png)
*Language model performance (averaged across position) on the multi-document QA task decreases as context grows longer.*

### Figure 7: Key-Value Retrieval Task Example
![Figure 7](../../../../_media/lost-in-middle-fig7.png)
*Example of the key-value retrieval task with 128-bit UUID keys and values.*

### Figure 8: Key-Value Context Length Modulation
![Figure 8](../../../../_media/lost-in-middle-fig8.png)
*Adding random key-value pairs increases length of the input context.*

### Figure 9: Key-Value Position Modulation
![Figure 9](../../../../_media/lost-in-middle-fig9.png)
*Re-ordering the key-value pairs does not affect the desired output.*

### Figure 10: Key-Value Retrieval Results
![Figure 10](../../../../_media/lost-in-middle-fig10.png)
*The effect of changing the input context length and the position of relevant information on key-value retrieval performance.*

### Figure 11: Encoder-Decoder Model Robustness
![Figure 11](../../../../_media/lost-in-middle-fig11.png)
*Encoder-decoder models (Flan-UL2 and Flan-T5-XXL) are relatively robust to changes in the position of relevant information.*

### Figure 12: Query-Aware Contextualization
![Figure 12](../../../../_media/lost-in-middle-fig12.png)
*Query-aware contextualization (placing the question before and after the documents) improves multi-document QA performance.*

### Figure 13: Instruction Fine-Tuning Effect
![Figure 13](../../../../_media/lost-in-middle-fig13.png)
*MPT-30B-Instruct compared against its base model. Both have a U-shaped curve.*

### Figure 14: Retriever Recall vs Model Performance
![Figure 14](../../../../_media/lost-in-middle-fig14.png)
*Model performance saturates long before retriever recall saturates, indicating models have difficulty using all retrieved information.*

### Figure 15: Unambiguous Questions Subset
![Figure 15](../../../../_media/lost-in-middle-fig15.png)
*Language model performance on an unambiguous subset of questions.*

### Figure 16: Randomized Distractor Order
![Figure 16](../../../../_media/lost-in-middle-fig16.png)
*Language model performance when randomizing the order of the distractors.*

### Figure 17: GPT-4 Performance
![Figure 17](../../../../_media/lost-in-middle-fig17.png)
*Although GPT-4 has higher absolute performance, its performance still degrades when relevant information occurs in the middle.*
